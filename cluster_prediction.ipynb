{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import random\n",
    "#import pymc as pm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"rocket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af7147",
   "metadata": {},
   "source": [
    "# CONFIG\n",
    "\n",
    "**Set sample to False if you want to run algos on entire dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'sample': False,\n",
    "    'sample_months': 1, # how many months to sample (for development)\n",
    "    'choose_month': 5, # month to choose for sampling // Note: overrides sample_months\n",
    "    'top_n_clusters': 8, # how many clusters to train (upper bound)\n",
    "    # For now, this is only implemented for the whole cluster models for brevity. \n",
    "    'start_at_cluster': 3, # all clusters with an id smaller than this value will be skipped! (lower bound to the line above) \n",
    "    'random_state': 123,\n",
    "    'target_col': 'departures',\n",
    "    'n_jobs': 4,  # gridsearch parallelization, might need to adjust based on your system\n",
    "    'ts_splits': 5, # TimeSeriesSplit number of splits\n",
    "    'ts_gap': 48,  # 2-day gap\n",
    "    'visualize_clusters': False\n",
    "}\n",
    "\n",
    "FEATURE_COLS = {\n",
    "    'categorical': ['isHoliday', 'has_kiosk', 'weather_cluster', 'workhours', 'commute', 'free', 'night'],\n",
    "    'drop': ['sum', 'weather_code', 'timestamp', 'station_name', 'arrivals', 'num_docks_available', 'num_ebikes_available', 'capacity', 'cluster', 'sunset', 'sunrise', 'year', 'hour_extract', 'precipitation', 'wind_gusts_10m', 'dayofyear', 'dayofweek', 'delta'],\n",
    "    'time': ['weekday', 'day', 'month', 'hour']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95c97e",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/final/df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e326de",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS[\"drop\"] = FEATURE_COLS[\"drop\"] + [col for col in df.columns if (col.startswith(\"var\") or col.startswith(\"avg\"))]\n",
    "\n",
    "# All remaining columns are considered numerical\n",
    "FEATURE_COLS['numerical'] = [col for col in df.columns if col not in ([CONFIG['target_col']] + FEATURE_COLS['categorical'] + FEATURE_COLS['drop'] + FEATURE_COLS['time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ca495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCE DATASET SIZE FOR DEVELOPMENT\n",
    "if CONFIG['sample']:\n",
    "    if CONFIG['choose_month'] is not None:\n",
    "        df = df[df['timestamp'].dt.month == CONFIG['choose_month']]\n",
    "        print(f'Chosen month: {CONFIG[\"choose_month\"]}')\n",
    "    else:\n",
    "        months = df['timestamp'].dt.month.unique()\n",
    "        random_month = random.sample(list(months), CONFIG['sample_months'])\n",
    "        df = df[df['timestamp'].dt.month.isin(random_month)]\n",
    "        print(f'Sampled {CONFIG[\"sample_months\"]} month(s): {random_month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7bc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee707dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do we need to update this to conform with the \"primary\" train-test-split? (The one at the very top)\n",
    "# Baseline-Performance for 0-heavy Data:\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df[df[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over all time cluster {i}: {baseline}\")\n",
    "print(\"---\")\n",
    "baseline_citywide_aggregate_df = df[[\"departures\",\"hour\",\"day\", \"month\"]].copy().groupby(['hour', 'day', 'month'], as_index=False)['departures'].sum()\n",
    "# Filter out rows where month == 12 and hour > 15, as we do not have weather data for these so they get implicitly dropped later on\n",
    "baseline_citywide_aggregate_df = baseline_citywide_aggregate_df[~((baseline_citywide_aggregate_df['month'] == 12) & (baseline_citywide_aggregate_df['hour'] > 15))]\n",
    "baseline = np.sqrt((sum(baseline_citywide_aggregate_df[\"departures\"]**2))/len(baseline_citywide_aggregate_df))\n",
    "print(f\"Baseline over all time whole city: {baseline}\")\n",
    "print(\"---\")\n",
    "df_may_subset = df[(df[\"month\"]==5) & (df[\"day\"]>=10) & (df[\"day\"]<=20)].copy()\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df_may_subset[df_may_subset[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over 10-20 may cluster {i}: {baseline}\")\n",
    "df_may_subset = df[(df[\"month\"]==5) & (df[\"day\"]>=21) & (df[\"day\"]<=30)].copy()\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df_may_subset[df_may_subset[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over 21-30 may cluster {i}: {baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85705d37",
   "metadata": {},
   "source": [
    "## Feature sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for features\n",
    "print(\"Categorical:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['categorical']))\n",
    "print(\"Drop:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['drop']))\n",
    "print(\"Time:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['time']))\n",
    "print(\"Numerical:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['numerical']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "sns.histplot(df[CONFIG['target_col']])\n",
    "plt.title(f'Distribution of {CONFIG[\"target_col\"]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6553df3",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ca232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_data(df, target_col, categorical_cols, numerical_cols, time_cols, drop_cols):\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"Dropped {len(df) - len(df_clean)} rows with NaN values.\")\n",
    "\n",
    "    # Keep datetime for visualization purposes if available\n",
    "    datetime_col = df_clean['timestamp'] if 'timestamp' in df_clean.columns else None\n",
    "    datetime_keeper = df_clean.copy()\n",
    "\n",
    "    # Drop columns defined in drop_cols\n",
    "    df_clean = df_clean.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # Split features and target\n",
    "    X = df_clean[categorical_cols + numerical_cols + time_cols]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=CONFIG['random_state'], shuffle=False)\n",
    "\n",
    "    train_indices = X_train.index\n",
    "    test_indices = X_test.index\n",
    "    datetime_train = datetime_keeper.loc[train_indices, 'timestamp'] if datetime_col is not None else None\n",
    "    datetime_test = datetime_keeper.loc[test_indices, 'timestamp'] if datetime_col is not None else None\n",
    "    datetime_col = [datetime_train, datetime_test] if datetime_train is not None else None\n",
    "   \n",
    "    return X_train, y_train, X_test, y_test, datetime_col\n",
    "\n",
    "# Test-apply data preparation\n",
    "X, y, X_test, y_test, datetime_col = prepare_data(df, CONFIG['target_col'], FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], FEATURE_COLS['drop'])\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set:\")\n",
    "print(f\"Features shape: {X_test.shape}\")\n",
    "print(f\"Target shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20b0e3",
   "metadata": {},
   "source": [
    "# Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sin_transformer(period):\n",
    "    def _transform(X_input): # X_input will be a 2D numpy array (n_samples, 1 feature)\n",
    "        # Extract the first (and only) column for calculation\n",
    "        data = X_input[:, 0]\n",
    "        # Perform transformation and ensure output is a 2D column vector\n",
    "        return np.sin(data / period * 2 * np.pi).reshape(-1, 1)\n",
    "\n",
    "    return FunctionTransformer(\n",
    "        _transform,\n",
    "        feature_names_out=\"one-to-one\", # This allows ColumnTransformer to get feature names\n",
    "        validate=True # Ensures input is 2D float numpy array and output is 2D\n",
    "    )\n",
    "\n",
    "def cos_transformer(period):\n",
    "    def _transform(X_input): # X_input will be a 2D numpy array (n_samples, 1 feature)\n",
    "        # Extract the first (and only) column for calculation\n",
    "        data = X_input[:, 0]\n",
    "        # Perform transformation and ensure output is a 2D column vector\n",
    "        return np.cos(data / period * 2 * np.pi).reshape(-1, 1)\n",
    "\n",
    "    return FunctionTransformer(\n",
    "        _transform,\n",
    "        feature_names_out=\"one-to-one\", # This allows ColumnTransformer to get feature names\n",
    "        validate=True # Ensures input is 2D float numpy array and output is 2D\n",
    "    )\n",
    "\n",
    "# Function to create model pipelines for each cluster\n",
    "def create_model_pipelines(categorical_cols, numerical_cols, time_cols, X):\n",
    "    \n",
    "    preprocessor_plain = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    preprocessor_onehot = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        ('time', OneHotEncoder(handle_unknown='ignore', sparse_output=False), time_cols),\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    # Prepared but currently not in use\n",
    "    preprocessor_sincos = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        ('sin_month', sin_transformer(12), ['month']),\n",
    "        ('sin_hour', sin_transformer(24), ['hour']),\n",
    "        ('sin_weekday', sin_transformer(7), ['weekday']),\n",
    "        ('cos_month', cos_transformer(12), ['month']),\n",
    "        ('cos_hour', cos_transformer(24), ['hour']),\n",
    "        ('cos_weekday', cos_transformer(7), ['weekday'])\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    numerical_indices = [X.columns.get_loc(c) for c in numerical_cols]\n",
    "\n",
    "    poly_transformer = ColumnTransformer([\n",
    "        (\"poly\", PolynomialFeatures(include_bias=False), numerical_indices)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    # Create pipelines\n",
    "    pipelines = {\n",
    "        'linear': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        \n",
    "        'lasso': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', Lasso(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        \n",
    "        'ridge': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', Ridge(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'polynomial': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('poly', poly_transformer),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        'decision_tree': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', DecisionTreeRegressor(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'random_forest': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', RandomForestRegressor(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'xgboost': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', xgb.XGBRegressor(objective='reg:squarederror', random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'gbm': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', GradientBoostingRegressor(random_state=CONFIG['random_state']))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Store preprocessor strategies\n",
    "    preprocessing_strategies = {\n",
    "        'plain': preprocessor_plain,\n",
    "        'onehot': preprocessor_onehot,\n",
    "        'sincos': preprocessor_sincos\n",
    "    }\n",
    "    \n",
    "    preprocessor_plain.strategy_name = 'Plain'\n",
    "    preprocessor_onehot.strategy_name = 'OneHot'  \n",
    "    preprocessor_sincos.strategy_name = 'SinCos'\n",
    "    \n",
    "    return pipelines, preprocessing_strategies\n",
    "\n",
    "# Function to get parameter grids\n",
    "def get_param_grids(preprocessing_strategies):\n",
    "    return {\n",
    "        'linear': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ]\n",
    "            },\n",
    "        \n",
    "        'lasso': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__alpha': np.logspace(-4,4,20),\n",
    "            'regressor__max_iter': [1000, 2000]\n",
    "        },\n",
    "        \n",
    "        'ridge': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__alpha': np.logspace(-4,4,20)\n",
    "        },\n",
    "\n",
    "        'polynomial': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'poly__poly__degree': [2, 3, 4]\n",
    "        },\n",
    "        \n",
    "        'decision_tree': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__max_depth': [3, 5, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10, 15, 20]\n",
    "        },\n",
    "        \n",
    "        'random_forest': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10, 15, 20]\n",
    "        },\n",
    "        \n",
    "        'xgboost': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [3, 6],\n",
    "            'regressor__learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        \n",
    "        'gbm': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [3, 6],\n",
    "            'regressor__learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f208423",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04542997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(model_name, pipeline, cluster_id):\n",
    "    reg = pipeline.named_steps['regressor']\n",
    "            \n",
    "    feat_names = pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "            \n",
    "    # build and sort df\n",
    "    coef_df = (\n",
    "        pd.DataFrame({'feature': feat_names, 'coefficient': reg.coef_})\n",
    "            .assign(abs_coef=lambda df: df.coefficient.abs())\n",
    "            .sort_values('abs_coef', ascending=False)\n",
    "            .drop(columns='abs_coef')\n",
    "        )\n",
    "\n",
    "    # largest coefficients first\n",
    "    coef_df = coef_df[::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(coef_df['coefficient'], coef_df['feature'], s=50, color='C0')\n",
    "    plt.axvline(0, linestyle='--', color='gray')\n",
    "    plt.title(f'{str(model_name).capitalize()} Coefficients (Cluster {cluster_id})')\n",
    "    plt.xlabel('Coefficient value')\n",
    "    plt.tick_params(axis='y', pad=5) # increase space around y ticks\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/training/{cluster_id}_{model_name}_coefficients_model_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "    return coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07299b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model_name, model, X, cluster_id):\n",
    "    # Extract the regressor from pipeline\n",
    "    regressor = None\n",
    "    for step_name, step in model.named_steps.items():\n",
    "        if hasattr(step, 'feature_importances_'):\n",
    "            regressor = step\n",
    "            break\n",
    "    \n",
    "    if regressor is None:\n",
    "        print(f\"Model {model_name} doesn't support feature importance.\")\n",
    "        return\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    try:\n",
    "        # Try to get preprocessed feature names\n",
    "        if 'preprocessing' in model.named_steps and hasattr(model['preprocessing'], 'get_feature_names_out'):\n",
    "            feature_names = model['preprocessing'].get_feature_names_out()\n",
    "        else:\n",
    "            # Fallback to original feature names or indices\n",
    "            feature_names = X.columns if hasattr(X, 'columns') else [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        # Ensure the lengths match\n",
    "        if len(feature_names) != len(regressor.feature_importances_):\n",
    "            print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match importances length ({len(regressor.feature_importances_)})\")\n",
    "            # Use indices as fallback\n",
    "            feature_names = [f\"feature_{i}\" for i in range(len(regressor.feature_importances_))]\n",
    "            \n",
    "        # Extract feature importances\n",
    "        importance = regressor.feature_importances_\n",
    "        \n",
    "        # Create DataFrame for better visualization\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/training/{cluster_id}_{model_name}_feature_importances.png')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Feature importance report for {model_name}:\")\n",
    "        print(feature_imp)\n",
    "        \n",
    "        return feature_imp\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_predictions(model_name, model, X, y, datetime_col, cluster_id):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    pred_df = pd.DataFrame({\n",
    "        'datetime': datetime_col,\n",
    "        'actual': y,\n",
    "        'predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sorted(pred_df['datetime']), pred_df['actual'], label='Actual', alpha=0.7)\n",
    "    plt.plot(sorted(pred_df['datetime']), pred_df['predicted'], label='Predicted', alpha=0.7)\n",
    "    plt.title(f'{model_name} - Actual vs Predicted (MSE: {mse:.2f}, R²: {r2:.2f})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if cluster_id == 'may_subset': # Magic happens here, don't question it\n",
    "        plt.savefig(f'figures/training/may_subset/{cluster_id}_{model_name}_actual_vs_predicted.png')\n",
    "    else:\n",
    "        plt.savefig(f'figures/training/{cluster_id}_{model_name}_actual_vs_predicted.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot residuals\n",
    "    pred_df['residual'] = pred_df['actual'] - pred_df['predicted']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(pred_df['predicted'], pred_df['residual'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title(f'{model_name} - Residuals Plot')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.tight_layout()\n",
    "    if cluster_id == 'may_subset':  \n",
    "        plt.savefig(f'figures/training/may_subset/{cluster_id}_{model_name}_residuals.png')\n",
    "    else:\n",
    "        plt.savefig(f'figures/training/{cluster_id}_{model_name}_residuals.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeModelMetricsOnDisc(name, subset: bool, results, cluster_id):\n",
    "    FILE = \"checkpoints/model_train_checkpointing.csv\"\n",
    "    file_exists = os.path.exists(FILE)\n",
    "\n",
    "    best_params = ' '.join(str(results['best_params']).replace('\\n', '').split())\n",
    "    time_string = '{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "\n",
    "    with open(FILE, 'a') as file:\n",
    "        if not file_exists:\n",
    "            file.write(\"timestamp,subset,model,cluster_id,best_params,best_score,rmse,mean_train_score\\n\")\n",
    "        file.write(f\"{time_string},{subset},{name},{cluster_id},\\\"{best_params}\\\",{results['best_score']},{results['rmse']},{results['mean_train_score']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a single model\n",
    "def train_evaluate_model(cluster_id, subset:bool, name, pipeline, param_grid, X, y, n_splits=CONFIG['ts_splits']):\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    \n",
    "    # Use TimeSeriesSplit for validation\n",
    "    tscv = TimeSeriesSplit(\n",
    "        n_splits=n_splits,\n",
    "        gap=CONFIG['ts_gap'])\n",
    "    \n",
    "    # GridSearch with time series split\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid,\n",
    "        cv=tscv, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=CONFIG['n_jobs'],\n",
    "        verbose=1,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    if 'preprocessing' in best_params.keys():\n",
    "        if hasattr(best_params['preprocessing'], 'strategy_name'):\n",
    "            preproc_name = best_params['preprocessing'].strategy_name\n",
    "    else:\n",
    "        preproc_name = 'Unknown'\n",
    "    \n",
    "    # Store results\n",
    "    best_model = grid_search.best_estimator_\n",
    "    mean_train_scores = -grid_search.cv_results_['mean_train_score']\n",
    "    mean_train_score = np.mean(mean_train_scores)\n",
    "    result = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': -grid_search.best_score_,  # Convert back to positive MSE\n",
    "        'rmse': np.sqrt(-grid_search.best_score_),\n",
    "        'mean_train_score': mean_train_score,\n",
    "    }\n",
    "    \n",
    "    print(f\"  Best parameters: {result['best_params']}\")\n",
    "    print(f\"  Preprocessing strategy: {preproc_name}\")\n",
    "    print(f\"  MSE: {result['best_score']:.4f}\")\n",
    "    print(f\"  RMSE: {result['rmse']:.4f}\")\n",
    "    print(f\"  Mean Train Score: {mean_train_score}\")\n",
    "    storeModelMetricsOnDisc(name, subset, result, cluster_id)\n",
    "\n",
    "    return best_model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875adcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cluster_data(cluster_id, df_processed):\n",
    "\n",
    "    cluster_df = df_processed[df_processed['cluster'] == cluster_id].copy()\n",
    "    return prepare_data(cluster_df, CONFIG['target_col'], FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], FEATURE_COLS['drop'])\n",
    "\n",
    "def train_cluster_models(cluster_id, X, y, models_to_train, subset: bool):\n",
    "\n",
    "    best_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    pipelines, preprocessing_strategies = create_model_pipelines(FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], X)\n",
    "    param_grids = get_param_grids(preprocessing_strategies)\n",
    "    \n",
    "    for model_name in models_to_train:\n",
    "        if model_name in pipelines:\n",
    "            best_models[model_name], results[model_name] = train_evaluate_model(\n",
    "                cluster_id, subset, model_name, pipelines[model_name], param_grids[model_name], X, y\n",
    "            )\n",
    "    \n",
    "    return best_models, results\n",
    "\n",
    "def create_comparison_df(results_dict):\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': list(results_dict.keys()),\n",
    "        'RMSE': [results_dict[m]['rmse'] for m in results_dict.keys()]\n",
    "    }).sort_values('RMSE')\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2e498",
   "metadata": {},
   "source": [
    "# Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879e149",
   "metadata": {},
   "source": [
    "## 1. Cluster Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91651622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_train_cluster = ['linear', 'lasso', 'ridge', 'polynomial', 'decision_tree', 'xgboost']\n",
    "models_to_train_cluster = ['linear', 'lasso', 'ridge', 'decision_tree', 'xgboost']\n",
    "\n",
    "unique_clusters = sorted(df['cluster'].unique().tolist())\n",
    "print(f\"Found {len(unique_clusters)} clusters: {unique_clusters}\")\n",
    "\n",
    "# # Store cluster results\n",
    "all_cluster_models = {}\n",
    "all_cluster_results = {}\n",
    "all_cluster_comparisons = {}\n",
    "\n",
    "# Train models for each cluster\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    print(cluster_id)\n",
    "    print(CONFIG['start_at_cluster'])\n",
    "    if cluster_id < CONFIG['start_at_cluster']:\n",
    "        print(cluster_id)\n",
    "        print(CONFIG['start_at_cluster'])\n",
    "        continue\n",
    "    print(f\"\\n{'='*50}\\nProcessing Cluster {cluster_id}\\n{'='*50}\")\n",
    "    \n",
    "    # Get data for this cluster\n",
    "    X_cluster, y_cluster, X_cluster_test, y_cluster_test, datetime_col = prepare_cluster_data(cluster_id, df)\n",
    "    \n",
    "    # Train models\n",
    "    models, results = train_cluster_models(cluster_id, X_cluster, y_cluster, models_to_train_cluster, subset=False)\n",
    "\n",
    "    # Store results\n",
    "    all_cluster_models[cluster_id] = models\n",
    "    all_cluster_results[cluster_id] = results\n",
    "    all_cluster_comparisons[cluster_id] = create_comparison_df(results)\n",
    "            \n",
    "    print(f\"\\nModel Comparison for Cluster {cluster_id}:\")\n",
    "    comparison_cluster = create_comparison_df(results)\n",
    "    print(comparison_cluster)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=comparison_cluster)\n",
    "    plt.title(f'Cluster {cluster_id} - Model Comparison (RMSE - lower is better)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/training/cluster_{cluster_id}_model_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nFeature Importance for {model_name} in Cluster {cluster_id}:\")\n",
    "            feature_importance = plot_feature_importance(model_name, models[model_name], X_cluster, cluster_id)\n",
    "\n",
    "    # \"feature importance\" for regression methods\n",
    "    for model_name in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nCoefficients for {model_name} in Cluster {cluster_id}:\")\n",
    "            coefficients = plot_coefficients(model_name, models[model_name], cluster_id)\n",
    "    \n",
    "    # Visualize best model predictions\n",
    "    best_model_name = comparison_cluster['Model'].iloc[0]\n",
    "    print(f\"Best model for Cluster {cluster_id}: {best_model_name} with validation RMSE: {comparison_cluster['RMSE'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Test best model on test set\n",
    "    y_cluster_pred = models[best_model_name].predict(X_cluster_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_cluster_test, y_cluster_pred))\n",
    "    print(f\"Best model on test set for Cluster {cluster_id}: {best_model_name} with test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(f\"\\nVisualizing predictions on test set (5% of data) for {best_model_name} in Cluster {cluster_id}:\")\n",
    "    pred_df = visualize_predictions(best_model_name, models[best_model_name], X_cluster_test, y_cluster_test, datetime_col[1], cluster_id) # datetime_col[1] are times corresponding to test set\n",
    "\n",
    "    if i == CONFIG['top_n_clusters']-1:\n",
    "        print(f\"Reached top {CONFIG['top_n_clusters']} clusters. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fe03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of best models across clusters\n",
    "summary_rows = []\n",
    "for cluster_id in all_cluster_comparisons:\n",
    "    best_model = all_cluster_comparisons[cluster_id].iloc[0]\n",
    "    summary_rows.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Best Model': best_model['Model'],\n",
    "        'RMSE': best_model['RMSE'],\n",
    "    })\n",
    "\n",
    "cluster_summary = pd.DataFrame(summary_rows).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nBest Models by Cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Plot cluster performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_summary_plot = cluster_summary.copy()\n",
    "cluster_summary_plot['Cluster_Model'] = cluster_summary_plot.apply(\n",
    "    lambda x: f\"Cluster {x['Cluster']}: {x['Best Model']}\", axis=1\n",
    ")\n",
    "sns.barplot(x='RMSE', y='Cluster_Model', data=cluster_summary_plot)\n",
    "plt.title('Best Model Performance by Cluster (RMSE - lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/training/best_model_performance_by_cluster.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b05184",
   "metadata": {},
   "source": [
    "## 2. Citywide Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train_city = ['linear', 'lasso', 'ridge', 'decision_tree', 'xgboost']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01986065",
   "metadata": {},
   "source": [
    "### Fully Pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "print(f\"Models to train (citywide): {models_to_train_city}\")\n",
    "\n",
    "current_feature_cols_citywide = copy.deepcopy(FEATURE_COLS)\n",
    "\n",
    "if 'cluster' not in current_feature_cols_citywide['drop']:\n",
    "    current_feature_cols_citywide['drop'].append('cluster')\n",
    "    print(\"Appended 'cluster' to drop cols.\")\n",
    "\n",
    "for cat_list_name in ['categorical', 'numerical', 'time']:\n",
    "    if cat_list_name in current_feature_cols_citywide and 'cluster' in current_feature_cols_citywide[cat_list_name]:\n",
    "        current_feature_cols_citywide[cat_list_name].remove('cluster')\n",
    "        print(f\"Removed 'cluster' from {cat_list_name} cols.\")\n",
    "\n",
    "X_city, y_city, X_city_test, y_city_test, datetime_col_city = prepare_data(\n",
    "    df,\n",
    "    CONFIG['target_col'],\n",
    "    current_feature_cols_citywide['categorical'],\n",
    "    current_feature_cols_citywide['numerical'],\n",
    "    current_feature_cols_citywide['time'],\n",
    "    current_feature_cols_citywide['drop']\n",
    ")\n",
    "\n",
    "print(f\"\\nCitywide training set features shape: {X_city.shape}\")\n",
    "print(f\"Citywide training set target shape: {y_city.shape}\")\n",
    "print(f\"Citywide test set features shape: {X_city_test.shape}\")\n",
    "print(f\"Citywide test set target shape: {y_city_test.shape}\")\n",
    "\n",
    "city_best_models = {}\n",
    "city_results = {}\n",
    "\n",
    "pipelines_city, preprocessing_strategies_city = create_model_pipelines(\n",
    "    current_feature_cols_citywide['categorical'],\n",
    "    current_feature_cols_citywide['numerical'],\n",
    "    current_feature_cols_citywide['time'],\n",
    "    X_city\n",
    ")\n",
    "param_grids_city = get_param_grids(preprocessing_strategies_city)\n",
    "\n",
    "print(f\"\\n{'='*50}\\nTraining models for the entire city\\n{'='*50}\")\n",
    "for model_name_item in models_to_train_city:\n",
    "    if model_name_item in pipelines_city:\n",
    "        city_best_models[model_name_item], city_results[model_name_item] = train_evaluate_model(\n",
    "            'citywide',  # Using 'citywide' as pseudo cluster_id\n",
    "            False,\n",
    "            model_name_item,\n",
    "            pipelines_city[model_name_item],\n",
    "            param_grids_city[model_name_item],\n",
    "            X_city,\n",
    "            y_city\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping {model_name_item} as it's not defined in pipelines_city\")\n",
    "\n",
    "# Create and display comparison DataFrame for citywide models\n",
    "if city_results:\n",
    "    city_comparison_df = create_comparison_df(city_results)\n",
    "    print(\"\\nModel Comparison Citywide:\")\n",
    "    print(city_comparison_df)\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=city_comparison_df.sort_values('RMSE', ascending=True))\n",
    "    plt.title('Citywide Model Comparison (RMSE - lower is better)')\n",
    "    plt.savefig('figures/training/citywide_model_comparison.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name_item in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name_item in city_best_models:\n",
    "            print(f\"\\nFeature Importance for {model_name_item} (Citywide):\")\n",
    "            plot_feature_importance(model_name_item, city_best_models[model_name_item], X_city, 'citywide')\n",
    "\n",
    "    # Show coefficients for linear models\n",
    "    for model_name_item in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name_item in city_best_models:\n",
    "            print(f\"\\nCoefficients for {model_name_item} (Citywide):\")\n",
    "            plot_coefficients(model_name_item, city_best_models[model_name_item], 'citywide')\n",
    "    \n",
    "    if not city_comparison_df.empty:\n",
    "        best_city_model_name = city_comparison_df['Model'].iloc[0]\n",
    "        best_city_model_rmse_val = city_comparison_df['RMSE'].iloc[0]\n",
    "        print(f\"\\nBest model for the City (based on validation RMSE): {best_city_model_name} with Validation RMSE: {best_city_model_rmse_val:.4f}\")\n",
    "\n",
    "        # Test best model on the citywide test set\n",
    "        y_city_pred_test = city_best_models[best_city_model_name].predict(X_city_test)\n",
    "        city_test_rmse = np.sqrt(mean_squared_error(y_city_test, y_city_pred_test))\n",
    "        r2_city_test = r2_score(y_city_test, y_city_pred_test)\n",
    "        print(f\"Best city model ({best_city_model_name}) on Test Set: RMSE: {city_test_rmse:.4f}, R²: {r2_city_test:.4f}\")\n",
    "\n",
    "        print(f\"\\nVisualizing predictions on test set for {best_city_model_name} (Citywide):\")\n",
    "        # datetime_col_city[0] is train datetime, datetime_col_city[1] is test datetime\n",
    "        visualize_predictions(best_city_model_name, city_best_models[best_city_model_name], X_city_test, y_city_test, datetime_col_city[1], 'citywide')\n",
    "    else:\n",
    "        print(\"No models for citywide\")\n",
    "else:\n",
    "    print(\"No results for citywide\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba511f",
   "metadata": {},
   "source": [
    "### Aggregate (City-Wide - the real deal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b151f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "print(f\"Models to train (citywide): {models_to_train_city}\")\n",
    "\n",
    "current_feature_cols_citywide = copy.deepcopy(FEATURE_COLS)\n",
    "\n",
    "if 'cluster' not in current_feature_cols_citywide['drop']:\n",
    "    current_feature_cols_citywide['drop'].append('cluster')\n",
    "    print(\"Appended 'cluster' to drop cols.\")\n",
    "\n",
    "for cat_list_name in ['categorical', 'numerical', 'time']:\n",
    "    if cat_list_name in current_feature_cols_citywide and 'cluster' in current_feature_cols_citywide[cat_list_name]:\n",
    "        current_feature_cols_citywide[cat_list_name].remove('cluster')\n",
    "        print(f\"Removed 'cluster' from {cat_list_name} cols.\")\n",
    "\n",
    "current_feature_cols_citywide['categorical'].remove('has_kiosk')\n",
    "current_feature_cols_citywide['categorical'].remove('weather_cluster')\n",
    "current_feature_cols_citywide['numerical'].remove('num_bikes_available')\n",
    "current_feature_cols_citywide['numerical'].remove('latitude')\n",
    "current_feature_cols_citywide['numerical'].remove('longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = ['hour', 'day', 'month']\n",
    "\n",
    "cols_to_average = ['temperature_2m', 'rain', 'snowfall', 'cloud_cover', 'wind_speed_10m']\n",
    "cols_to_pick_first = ['weekday', 'isHoliday', 'workhours', 'commute','free','night', 'timestamp']\n",
    "\n",
    "# Build aggregation dictionary\n",
    "agg_dict = {\n",
    "    'departures': 'sum',\n",
    "    **{col: 'mean' for col in cols_to_average},\n",
    "    **{col: 'first' for col in cols_to_pick_first}\n",
    "}\n",
    "\n",
    "# Apply aggregation\n",
    "aggregated_df = df.groupby(group_keys).agg(agg_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc04c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_city, y_city, X_city_test, y_city_test, datetime_col_city = prepare_data(\n",
    "    aggregated_df,\n",
    "    CONFIG['target_col'],\n",
    "    current_feature_cols_citywide['categorical'],\n",
    "    current_feature_cols_citywide['numerical'],\n",
    "    current_feature_cols_citywide['time'],\n",
    "    current_feature_cols_citywide['drop']\n",
    ")\n",
    "\n",
    "print(f\"\\nCitywide training set features shape: {X_city.shape}\")\n",
    "print(f\"Citywide training set target shape: {y_city.shape}\")\n",
    "print(f\"Citywide test set features shape: {X_city_test.shape}\")\n",
    "print(f\"Citywide test set target shape: {y_city_test.shape}\")\n",
    "\n",
    "city_best_models = {}\n",
    "city_results = {}\n",
    "\n",
    "pipelines_city, preprocessing_strategies_city = create_model_pipelines(\n",
    "    current_feature_cols_citywide['categorical'],\n",
    "    current_feature_cols_citywide['numerical'],\n",
    "    current_feature_cols_citywide['time'],\n",
    "    X_city\n",
    ")\n",
    "param_grids_city = get_param_grids(preprocessing_strategies_city)\n",
    "\n",
    "print(f\"\\n{'='*50}\\nTraining models for the entire city\\n{'='*50}\")\n",
    "for model_name_item in models_to_train_city:\n",
    "    if model_name_item in pipelines_city:\n",
    "        city_best_models[model_name_item], city_results[model_name_item] = train_evaluate_model(\n",
    "            'citywide-aggregate',  # Using 'citywide' as pseudo cluster_id\n",
    "            False,\n",
    "            model_name_item,\n",
    "            pipelines_city[model_name_item],\n",
    "            param_grids_city[model_name_item],\n",
    "            X_city,\n",
    "            y_city\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping {model_name_item} as it's not defined in pipelines_city\")\n",
    "\n",
    "# Create and display comparison DataFrame for citywide models\n",
    "if city_results:\n",
    "    city_comparison_df = create_comparison_df(city_results)\n",
    "    print(\"\\nModel Comparison Citywide:\")\n",
    "    print(city_comparison_df)\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=city_comparison_df.sort_values('RMSE', ascending=True))\n",
    "    plt.title('Citywide Model Comparison (RMSE - lower is better)')\n",
    "    plt.savefig('figures/training/citywide_model_aggregate_comparison.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name_item in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name_item in city_best_models:\n",
    "            print(f\"\\nFeature Importance for {model_name_item} (Citywide):\")\n",
    "            plot_feature_importance(model_name_item, city_best_models[model_name_item], X_city, 'citywide-aggregate')\n",
    "\n",
    "    # Show coefficients for linear models\n",
    "    for model_name_item in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name_item in city_best_models:\n",
    "            print(f\"\\nCoefficients for {model_name_item} (Citywide):\")\n",
    "            plot_coefficients(model_name_item, city_best_models[model_name_item], 'citywide-aggregate')\n",
    "    \n",
    "    if not city_comparison_df.empty:\n",
    "        best_city_model_name = city_comparison_df['Model'].iloc[0]\n",
    "        best_city_model_rmse_val = city_comparison_df['RMSE'].iloc[0]\n",
    "        print(f\"\\nBest model for the City (based on validation RMSE): {best_city_model_name} with Validation RMSE: {best_city_model_rmse_val:.4f}\")\n",
    "\n",
    "        # Test best model on the citywide test set\n",
    "        y_city_pred_test = city_best_models[best_city_model_name].predict(X_city_test)\n",
    "        city_test_rmse = np.sqrt(mean_squared_error(y_city_test, y_city_pred_test))\n",
    "        r2_city_test = r2_score(y_city_test, y_city_pred_test)\n",
    "        print(f\"Best city model ({best_city_model_name}) on Test Set: RMSE: {city_test_rmse:.4f}, R²: {r2_city_test:.4f}\")\n",
    "\n",
    "        print(f\"\\nVisualizing predictions on test set for {best_city_model_name} (Citywide):\")\n",
    "        # datetime_col_city[0] is train datetime, datetime_col_city[1] is test datetime\n",
    "        visualize_predictions(best_city_model_name, city_best_models[best_city_model_name], X_city_test, y_city_test, datetime_col_city[1], 'citywide-aggregate')\n",
    "    else:\n",
    "        print(\"No models for citywide\")\n",
    "else:\n",
    "    print(\"No results for citywide\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae469c",
   "metadata": {},
   "source": [
    "## 3. Subset Models (May 10-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this obviously only works if the df is not sampled via CONFIG['sample'] = True\n",
    "\n",
    "df_may = df[\n",
    "    (df['timestamp'].dt.month == 5) &\n",
    "    (df['timestamp'].dt.day >= 10) & \n",
    "    (df['timestamp'].dt.day <= 20)\n",
    "].copy()\n",
    "\n",
    "print(f\"Shape of df_may_subset: {df_may.shape}\")\n",
    "df_may.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c10d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest first to detect potential errors early\n",
    "models_to_train_cluster_may = ['random_forest', 'linear', 'lasso', 'ridge', 'decision_tree', 'xgboost']\n",
    "\n",
    "unique_clusters = sorted(df_may['cluster'].unique().tolist())\n",
    "print(f\"Found {len(unique_clusters)} clusters: {unique_clusters}\")\n",
    "\n",
    "# # Store cluster results\n",
    "all_cluster_models = {}\n",
    "all_cluster_results = {}\n",
    "all_cluster_comparisons = {}\n",
    "\n",
    "# Train models for each cluster\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    print(f\"\\n{'='*50}\\nProcessing Cluster {cluster_id}\\n{'='*50}\")\n",
    "    \n",
    "    # Get data for this cluster\n",
    "    X_cluster, y_cluster, X_cluster_test, y_cluster_test, datetime_col = prepare_cluster_data(cluster_id, df_may)\n",
    "    print(f\"Cluster size: {len(X_cluster)} records\")\n",
    "    \n",
    "    # Train models\n",
    "    models, results = train_cluster_models(cluster_id, X_cluster, y_cluster, models_to_train_cluster, subset=True)\n",
    "\n",
    "    # Store results\n",
    "    all_cluster_models[cluster_id] = models\n",
    "    all_cluster_results[cluster_id] = results\n",
    "    all_cluster_comparisons[cluster_id] = create_comparison_df(results)\n",
    "            \n",
    "    print(f\"\\nModel Comparison for Cluster {cluster_id}:\")\n",
    "    comparison_cluster = create_comparison_df(results)\n",
    "    print(comparison_cluster)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=comparison_cluster)\n",
    "    plt.title(f'Cluster {cluster_id} - Model Comparison (RMSE - lower is better)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/training/may_subset/cluster_{cluster_id}_model_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nFeature Importance for {model_name} in Cluster {cluster_id}:\")\n",
    "            feature_importance = plot_feature_importance(model_name, models[model_name], X_cluster, 'may_subset')\n",
    "\n",
    "    # \"feature importance\" for regression methods\n",
    "    for model_name in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nCoefficients for {model_name} in Cluster {cluster_id}:\")\n",
    "            coefficients = plot_coefficients(model_name, models[model_name], cluster_id)\n",
    "    \n",
    "    # Visualize best model predictions\n",
    "    best_model_name = comparison_cluster['Model'].iloc[0]\n",
    "    print(f\"Best model for Cluster {cluster_id}: {best_model_name} with validation RMSE: {comparison_cluster['RMSE'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Test best model on test set\n",
    "    y_cluster_pred = models[best_model_name].predict(X_cluster_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_cluster_test, y_cluster_pred))\n",
    "    print(f\"Best model on test set for Cluster {cluster_id}: {best_model_name} with test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(f\"\\nVisualizing predictions on test set (5% of data) for {best_model_name} in Cluster {cluster_id}:\")\n",
    "    pred_df = visualize_predictions(best_model_name, models[best_model_name], X_cluster_test, y_cluster_test, datetime_col[1], 'may_subset') # datetime_col[1] are times corresponding to test set\n",
    "\n",
    "    if i == CONFIG['top_n_clusters']-1:\n",
    "        print(f\"Reached top {CONFIG['top_n_clusters']} clusters. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of best models across clusters\n",
    "summary_rows = []\n",
    "for cluster_id in all_cluster_comparisons:\n",
    "    best_model = all_cluster_comparisons[cluster_id].iloc[0]\n",
    "    summary_rows.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Best Model': best_model['Model'],\n",
    "        'RMSE': best_model['RMSE'],\n",
    "    })\n",
    "\n",
    "cluster_summary = pd.DataFrame(summary_rows).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nBest Models by Cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Plot cluster performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_summary_plot = cluster_summary.copy()\n",
    "cluster_summary_plot['Cluster_Model'] = cluster_summary_plot.apply(\n",
    "    lambda x: f\"Cluster {x['Cluster']}: {x['Best Model']}\", axis=1\n",
    ")\n",
    "sns.barplot(x='RMSE', y='Cluster_Model', data=cluster_summary_plot)\n",
    "plt.title('Best Model Performance by Cluster (RMSE - lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afde712",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
