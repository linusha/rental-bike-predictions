{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import random\n",
    "#import pymc as pm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"rocket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af7147",
   "metadata": {},
   "source": [
    "# CONFIG\n",
    "\n",
    "**Set sample to False if you want to run algos on entire dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'sample': False,\n",
    "    'sample_months': 1, # how many months to sample (for development)\n",
    "    'choose_month': 5, # month to choose for sampling // Note: overrides sample_months\n",
    "    'top_n_clusters': 8, # how many clusters to train (upper bound)\n",
    "    # For now, this is only implemented for the whole cluster models for brevity. \n",
    "    'start_at_cluster': 3, # all clusters with an id smaller than this value will be skipped! (lower bound to the line above) \n",
    "    'random_state': 123,\n",
    "    'target_col': 'departures',\n",
    "    'n_jobs': 4,  # gridsearch parallelization, might need to adjust based on your system\n",
    "    'ts_splits': 5, # TimeSeriesSplit number of splits\n",
    "    'ts_gap': 48,  # 2-day gap\n",
    "    'visualize_clusters': False\n",
    "}\n",
    "\n",
    "FEATURE_COLS = {\n",
    "    'categorical': ['isHoliday', 'has_kiosk', 'weather_cluster', 'workhours', 'commute', 'free', 'night'],\n",
    "    'drop': ['sum', 'weather_code', 'timestamp', 'station_name', 'arrivals', 'num_docks_available', 'num_ebikes_available', 'capacity', 'cluster', 'sunset', 'sunrise', 'year', 'hour_extract', 'precipitation', 'wind_gusts_10m', 'dayofyear', 'dayofweek', 'delta'],\n",
    "    'time': ['weekday', 'day', 'month', 'hour']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95c97e",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/final/df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e326de",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS[\"drop\"] = FEATURE_COLS[\"drop\"] + [col for col in df.columns if (col.startswith(\"var\") or col.startswith(\"avg\"))]\n",
    "\n",
    "# All remaining columns are considered numerical\n",
    "FEATURE_COLS['numerical'] = [col for col in df.columns if col not in ([CONFIG['target_col']] + FEATURE_COLS['categorical'] + FEATURE_COLS['drop'] + FEATURE_COLS['time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ca495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCE DATASET SIZE FOR DEVELOPMENT\n",
    "if CONFIG['sample']:\n",
    "    if CONFIG['choose_month'] is not None:\n",
    "        df = df[df['timestamp'].dt.month == CONFIG['choose_month']]\n",
    "        print(f'Chosen month: {CONFIG[\"choose_month\"]}')\n",
    "    else:\n",
    "        months = df['timestamp'].dt.month.unique()\n",
    "        random_month = random.sample(list(months), CONFIG['sample_months'])\n",
    "        df = df[df['timestamp'].dt.month.isin(random_month)]\n",
    "        print(f'Sampled {CONFIG[\"sample_months\"]} month(s): {random_month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7bc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee707dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do we need to update this to conform with the \"primary\" train-test-split? (The one at the very top)\n",
    "# Baseline-Performance for 0-heavy Data:\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df[df[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over all time cluster {i}: {baseline}\")\n",
    "print(\"---\")\n",
    "baseline_citywide_aggregate_df = df[[\"departures\",\"hour\",\"day\", \"month\"]].copy().groupby(['hour', 'day', 'month'], as_index=False)['departures'].sum()\n",
    "# Filter out rows where month == 12 and hour > 15, as we do not have weather data for these so they get implicitly dropped later on\n",
    "baseline_citywide_aggregate_df = baseline_citywide_aggregate_df[~((baseline_citywide_aggregate_df['month'] == 12) & (baseline_citywide_aggregate_df['hour'] > 15))]\n",
    "baseline = np.sqrt((sum(baseline_citywide_aggregate_df[\"departures\"]**2))/len(baseline_citywide_aggregate_df))\n",
    "print(f\"Baseline over all time whole city: {baseline}\")\n",
    "print(\"---\")\n",
    "df_may_subset = df[(df[\"month\"]==5) & (df[\"day\"]>=10) & (df[\"day\"]<=20)].copy()\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df_may_subset[df_may_subset[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over 10-20 may cluster {i}: {baseline}\")\n",
    "df_may_subset = df[(df[\"month\"]==5) & (df[\"day\"]>=21) & (df[\"day\"]<=30)].copy()\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df_may_subset[df_may_subset[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over 21-30 may cluster {i}: {baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85705d37",
   "metadata": {},
   "source": [
    "## Feature sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for features\n",
    "print(\"Categorical:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['categorical']))\n",
    "print(\"Drop:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['drop']))\n",
    "print(\"Time:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['time']))\n",
    "print(\"Numerical:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['numerical']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "sns.histplot(df[CONFIG['target_col']])\n",
    "plt.title(f'Distribution of {CONFIG[\"target_col\"]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6553df3",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ca232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_data(df, target_col, categorical_cols, numerical_cols, time_cols, drop_cols):\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"Dropped {len(df) - len(df_clean)} rows with NaN values.\")\n",
    "\n",
    "    # Keep datetime for visualization purposes if available\n",
    "    datetime_col = df_clean['timestamp'] if 'timestamp' in df_clean.columns else None\n",
    "    datetime_keeper = df_clean.copy()\n",
    "\n",
    "    # Drop columns defined in drop_cols\n",
    "    df_clean = df_clean.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # Split features and target\n",
    "    X = df_clean[categorical_cols + numerical_cols + time_cols]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=CONFIG['random_state'], shuffle=False)\n",
    "\n",
    "    train_indices = X_train.index\n",
    "    test_indices = X_test.index\n",
    "    datetime_train = datetime_keeper.loc[train_indices, 'timestamp'] if datetime_col is not None else None\n",
    "    datetime_test = datetime_keeper.loc[test_indices, 'timestamp'] if datetime_col is not None else None\n",
    "    datetime_col = [datetime_train, datetime_test] if datetime_train is not None else None\n",
    "   \n",
    "    return X_train, y_train, X_test, y_test, datetime_col\n",
    "\n",
    "# Test-apply data preparation\n",
    "X, y, X_test, y_test, datetime_col = prepare_data(df, CONFIG['target_col'], FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], FEATURE_COLS['drop'])\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set:\")\n",
    "print(f\"Features shape: {X_test.shape}\")\n",
    "print(f\"Target shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20b0e3",
   "metadata": {},
   "source": [
    "# Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sin_transformer(period):\n",
    "    def _transform(X_input): # X_input will be a 2D numpy array (n_samples, 1 feature)\n",
    "        # Extract the first (and only) column for calculation\n",
    "        data = X_input[:, 0]\n",
    "        # Perform transformation and ensure output is a 2D column vector\n",
    "        return np.sin(data / period * 2 * np.pi).reshape(-1, 1)\n",
    "\n",
    "    return FunctionTransformer(\n",
    "        _transform,\n",
    "        feature_names_out=\"one-to-one\", # This allows ColumnTransformer to get feature names\n",
    "        validate=True # Ensures input is 2D float numpy array and output is 2D\n",
    "    )\n",
    "\n",
    "def cos_transformer(period):\n",
    "    def _transform(X_input): # X_input will be a 2D numpy array (n_samples, 1 feature)\n",
    "        # Extract the first (and only) column for calculation\n",
    "        data = X_input[:, 0]\n",
    "        # Perform transformation and ensure output is a 2D column vector\n",
    "        return np.cos(data / period * 2 * np.pi).reshape(-1, 1)\n",
    "\n",
    "    return FunctionTransformer(\n",
    "        _transform,\n",
    "        feature_names_out=\"one-to-one\", # This allows ColumnTransformer to get feature names\n",
    "        validate=True # Ensures input is 2D float numpy array and output is 2D\n",
    "    )\n",
    "\n",
    "# Function to create model pipelines for each cluster\n",
    "def create_model_pipelines(categorical_cols, numerical_cols, time_cols, X):\n",
    "    \n",
    "    preprocessor_plain = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    preprocessor_onehot = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        ('time', OneHotEncoder(handle_unknown='ignore', sparse_output=False), time_cols),\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    # Prepared but currently not in use\n",
    "    preprocessor_sincos = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        ('sin_month', sin_transformer(12), ['month']),\n",
    "        ('sin_hour', sin_transformer(24), ['hour']),\n",
    "        ('sin_weekday', sin_transformer(7), ['weekday']),\n",
    "        ('cos_month', cos_transformer(12), ['month']),\n",
    "        ('cos_hour', cos_transformer(24), ['hour']),\n",
    "        ('cos_weekday', cos_transformer(7), ['weekday'])\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    numerical_indices = [X.columns.get_loc(c) for c in numerical_cols]\n",
    "\n",
    "    poly_transformer = ColumnTransformer([\n",
    "        (\"poly\", PolynomialFeatures(include_bias=False), numerical_indices)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    # Create pipelines\n",
    "    pipelines = {\n",
    "        'linear': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        \n",
    "        'lasso': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', Lasso(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        \n",
    "        'ridge': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', Ridge(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'polynomial': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('poly', poly_transformer),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        'decision_tree': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', DecisionTreeRegressor(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'random_forest': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', RandomForestRegressor(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'xgboost': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', xgb.XGBRegressor(objective='reg:squarederror', random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'gbm': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', GradientBoostingRegressor(random_state=CONFIG['random_state']))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Store preprocessor strategies\n",
    "    preprocessing_strategies = {\n",
    "        'plain': preprocessor_plain,\n",
    "        'onehot': preprocessor_onehot,\n",
    "        'sincos': preprocessor_sincos\n",
    "    }\n",
    "    \n",
    "    preprocessor_plain.strategy_name = 'Plain'\n",
    "    preprocessor_onehot.strategy_name = 'OneHot'  \n",
    "    preprocessor_sincos.strategy_name = 'SinCos'\n",
    "    \n",
    "    return pipelines, preprocessing_strategies\n",
    "\n",
    "# Function to get parameter grids\n",
    "def get_param_grids(preprocessing_strategies):\n",
    "    return {\n",
    "        'linear': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ]\n",
    "            },\n",
    "        \n",
    "        'lasso': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__alpha': np.logspace(-4,4,20),\n",
    "            'regressor__max_iter': [1000, 2000]\n",
    "        },\n",
    "        \n",
    "        'ridge': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__alpha': np.logspace(-4,4,20)\n",
    "        },\n",
    "\n",
    "        'polynomial': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'poly__poly__degree': [2, 3, 4]\n",
    "        },\n",
    "        \n",
    "        'decision_tree': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__max_depth': [3, 5, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10, 15, 20]\n",
    "        },\n",
    "        \n",
    "        'random_forest': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10, 15, 20]\n",
    "        },\n",
    "        \n",
    "        'xgboost': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [3, 6],\n",
    "            'regressor__learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        \n",
    "        'gbm': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [3, 6],\n",
    "            'regressor__learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f208423",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04542997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(model_name, pipeline, cluster_id):\n",
    "    reg = pipeline.named_steps['regressor']\n",
    "            \n",
    "    feat_names = pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "            \n",
    "    # build and sort df\n",
    "    coef_df = (\n",
    "        pd.DataFrame({'feature': feat_names, 'coefficient': reg.coef_})\n",
    "            .assign(abs_coef=lambda df: df.coefficient.abs())\n",
    "            .sort_values('abs_coef', ascending=False)\n",
    "            .drop(columns='abs_coef')\n",
    "        )\n",
    "\n",
    "    # largest coefficients first\n",
    "    coef_df = coef_df[::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(coef_df['coefficient'], coef_df['feature'], s=50, color='C0')\n",
    "    plt.axvline(0, linestyle='--', color='gray')\n",
    "    plt.title(f'{str(model_name).capitalize()} Coefficients (Cluster {cluster_id})')\n",
    "    plt.xlabel('Coefficient value')\n",
    "    plt.tick_params(axis='y', pad=5) # increase space around y ticks\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/training/{cluster_id}_{model_name}_coefficients_model_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "    return coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07299b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model_name, model, X, cluster_id):\n",
    "    # Extract the regressor from pipeline\n",
    "    regressor = None\n",
    "    for step_name, step in model.named_steps.items():\n",
    "        if hasattr(step, 'feature_importances_'):\n",
    "            regressor = step\n",
    "            break\n",
    "    \n",
    "    if regressor is None:\n",
    "        print(f\"Model {model_name} doesn't support feature importance.\")\n",
    "        return\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    try:\n",
    "        # Try to get preprocessed feature names\n",
    "        if 'preprocessing' in model.named_steps and hasattr(model['preprocessing'], 'get_feature_names_out'):\n",
    "            feature_names = model['preprocessing'].get_feature_names_out()\n",
    "        else:\n",
    "            # Fallback to original feature names or indices\n",
    "            feature_names = X.columns if hasattr(X, 'columns') else [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        # Ensure the lengths match\n",
    "        if len(feature_names) != len(regressor.feature_importances_):\n",
    "            print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match importances length ({len(regressor.feature_importances_)})\")\n",
    "            # Use indices as fallback\n",
    "            feature_names = [f\"feature_{i}\" for i in range(len(regressor.feature_importances_))]\n",
    "            \n",
    "        # Extract feature importances\n",
    "        importance = regressor.feature_importances_\n",
    "        \n",
    "        # Create DataFrame for better visualization\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/training/{cluster_id}_{model_name}_feature_importances.png')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Feature importance report for {model_name}:\")\n",
    "        print(feature_imp)\n",
    "        \n",
    "        return feature_imp\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model_name, model, X, y, datetime_col, cluster_id):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    pred_df = pd.DataFrame({\n",
    "        'datetime': datetime_col,\n",
    "        'actual': y,\n",
    "        'predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(pred_df['datetime'], pred_df['actual'], label='Actual', alpha=0.7)\n",
    "    plt.plot(pred_df['datetime'], pred_df['predicted'], label='Predicted', alpha=0.7)\n",
    "    plt.title(f'{model_name} - Actual vs Predicted (MSE: {mse:.2f}, RÂ²: {r2:.2f})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if cluster_id == 'may_subset': # Magic happens here, don't question it\n",
    "        plt.savefig(f'figures/training/may_subset/{cluster_id}_{model_name}_actual_vs_predicted.png')\n",
    "    else:\n",
    "        plt.savefig(f'figures/training/{cluster_id}_{model_name}_actual_vs_predicted.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot residuals\n",
    "    pred_df['residual'] = pred_df['actual'] - pred_df['predicted']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(pred_df['predicted'], pred_df['residual'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title(f'{model_name} - Residuals Plot')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.tight_layout()\n",
    "    if cluster_id == 'may_subset':  \n",
    "        plt.savefig(f'figures/training/may_subset/{cluster_id}_{model_name}_residuals.png')\n",
    "    else:\n",
    "        plt.savefig(f'figures/training/{cluster_id}_{model_name}_residuals.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeModelMetricsOnDisc(name, subset: bool, results, cluster_id):\n",
    "    FILE = \"checkpoints/model_train_checkpointing.csv\"\n",
    "    file_exists = os.path.exists(FILE)\n",
    "\n",
    "    best_params = ' '.join(str(results['best_params']).replace('\\n', '').split())\n",
    "    time_string = '{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "\n",
    "    with open(FILE, 'a') as file:\n",
    "        if not file_exists:\n",
    "            file.write(\"timestamp,subset,model,cluster_id,best_params,best_score,rmse,mean_train_score\\n\")\n",
    "        file.write(f\"{time_string},{subset},{name},{cluster_id},\\\"{best_params}\\\",{results['best_score']},{results['rmse']},{results['mean_train_score']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a single model\n",
    "def train_evaluate_model(cluster_id, subset:bool, name, pipeline, param_grid, X, y, n_splits=CONFIG['ts_splits']):\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    \n",
    "    # Use TimeSeriesSplit for validation\n",
    "    tscv = TimeSeriesSplit(\n",
    "        n_splits=n_splits,\n",
    "        gap=CONFIG['ts_gap'])\n",
    "    \n",
    "    # GridSearch with time series split\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid,\n",
    "        cv=tscv, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=CONFIG['n_jobs'],\n",
    "        verbose=1,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    if 'preprocessing' in best_params.keys():\n",
    "        if hasattr(best_params['preprocessing'], 'strategy_name'):\n",
    "            preproc_name = best_params['preprocessing'].strategy_name\n",
    "    else:\n",
    "        preproc_name = 'Unknown'\n",
    "    \n",
    "    # Store results\n",
    "    best_model = grid_search.best_estimator_\n",
    "    mean_train_scores = -grid_search.cv_results_['mean_train_score']\n",
    "    mean_train_score = np.mean(mean_train_scores)\n",
    "    result = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': -grid_search.best_score_,  # Convert back to positive MSE\n",
    "        'rmse': np.sqrt(-grid_search.best_score_),\n",
    "        'mean_train_score': mean_train_score,\n",
    "    }\n",
    "    \n",
    "    print(f\"  Best parameters: {result['best_params']}\")\n",
    "    print(f\"  Preprocessing strategy: {preproc_name}\")\n",
    "    print(f\"  MSE: {result['best_score']:.4f}\")\n",
    "    print(f\"  RMSE: {result['rmse']:.4f}\")\n",
    "    print(f\"  Mean Train Score: {mean_train_score}\")\n",
    "    storeModelMetricsOnDisc(name, subset, result, cluster_id)\n",
    "\n",
    "    return best_model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875adcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cluster_data(cluster_id, df_processed):\n",
    "\n",
    "    cluster_df = df_processed[df_processed['cluster'] == cluster_id].copy()\n",
    "    return prepare_data(cluster_df, CONFIG['target_col'], FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], FEATURE_COLS['drop'])\n",
    "\n",
    "def train_cluster_models(cluster_id, X, y, models_to_train, subset: bool):\n",
    "\n",
    "    best_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    pipelines, preprocessing_strategies = create_model_pipelines(FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], X)\n",
    "    param_grids = get_param_grids(preprocessing_strategies)\n",
    "    \n",
    "    for model_name in models_to_train:\n",
    "        if model_name in pipelines:\n",
    "            best_models[model_name], results[model_name] = train_evaluate_model(\n",
    "                cluster_id, subset, model_name, pipelines[model_name], param_grids[model_name], X, y\n",
    "            )\n",
    "    \n",
    "    return best_models, results\n",
    "\n",
    "def create_comparison_df(results_dict):\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': list(results_dict.keys()),\n",
    "        'RMSE': [results_dict[m]['rmse'] for m in results_dict.keys()]\n",
    "    }).sort_values('RMSE')\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2e498",
   "metadata": {},
   "source": [
    "# Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879e149",
   "metadata": {},
   "source": [
    "## 1. Cluster Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91651622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_train_cluster = ['linear', 'lasso', 'ridge', 'polynomial', 'decision_tree', 'xgboost']\n",
    "models_to_train_cluster = ['linear', 'lasso', 'ridge', 'decision_tree', 'xgboost']\n",
    "\n",
    "unique_clusters = sorted(df['cluster'].unique().tolist())\n",
    "print(f\"Found {len(unique_clusters)} clusters: {unique_clusters}\")\n",
    "\n",
    "# # Store cluster results\n",
    "all_cluster_models = {}\n",
    "all_cluster_results = {}\n",
    "all_cluster_comparisons = {}\n",
    "\n",
    "# Train models for each cluster\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    print(cluster_id)\n",
    "    print(CONFIG['start_at_cluster'])\n",
    "    if cluster_id < CONFIG['start_at_cluster']:\n",
    "        print(cluster_id)\n",
    "        print(CONFIG['start_at_cluster'])\n",
    "        continue\n",
    "    print(f\"\\n{'='*50}\\nProcessing Cluster {cluster_id}\\n{'='*50}\")\n",
    "    \n",
    "    # Get data for this cluster\n",
    "    X_cluster, y_cluster, X_cluster_test, y_cluster_test, datetime_col = prepare_cluster_data(cluster_id, df)\n",
    "    \n",
    "    # Train models\n",
    "    models, results = train_cluster_models(cluster_id, X_cluster, y_cluster, models_to_train_cluster, subset=False)\n",
    "\n",
    "    # Store results\n",
    "    all_cluster_models[cluster_id] = models\n",
    "    all_cluster_results[cluster_id] = results\n",
    "    all_cluster_comparisons[cluster_id] = create_comparison_df(results)\n",
    "            \n",
    "    print(f\"\\nModel Comparison for Cluster {cluster_id}:\")\n",
    "    comparison_cluster = create_comparison_df(results)\n",
    "    print(comparison_cluster)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=comparison_cluster)\n",
    "    plt.title(f'Cluster {cluster_id} - Model Comparison (RMSE - lower is better)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/training/cluster_{cluster_id}_model_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nFeature Importance for {model_name} in Cluster {cluster_id}:\")\n",
    "            feature_importance = plot_feature_importance(model_name, models[model_name], X_cluster, cluster_id)\n",
    "\n",
    "    # \"feature importance\" for regression methods\n",
    "    for model_name in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nCoefficients for {model_name} in Cluster {cluster_id}:\")\n",
    "            coefficients = plot_coefficients(model_name, models[model_name], cluster_id)\n",
    "    \n",
    "    # Visualize best model predictions\n",
    "    best_model_name = comparison_cluster['Model'].iloc[0]\n",
    "    print(f\"Best model for Cluster {cluster_id}: {best_model_name} with validation RMSE: {comparison_cluster['RMSE'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Test best model on test set\n",
    "    y_cluster_pred = models[best_model_name].predict(X_cluster_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_cluster_test, y_cluster_pred))\n",
    "    print(f\"Best model on test set for Cluster {cluster_id}: {best_model_name} with test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(f\"\\nVisualizing predictions on test set (5% of data) for {best_model_name} in Cluster {cluster_id}:\")\n",
    "    pred_df = visualize_predictions(best_model_name, models[best_model_name], X_cluster_test, y_cluster_test, datetime_col[1], cluster_id) # datetime_col[1] are times corresponding to test set\n",
    "\n",
    "    if i == CONFIG['top_n_clusters']-1:\n",
    "        print(f\"Reached top {CONFIG['top_n_clusters']} clusters. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fe03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of best models across clusters\n",
    "summary_rows = []\n",
    "for cluster_id in all_cluster_comparisons:\n",
    "    best_model = all_cluster_comparisons[cluster_id].iloc[0]\n",
    "    summary_rows.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Best Model': best_model['Model'],\n",
    "        'RMSE': best_model['RMSE'],\n",
    "    })\n",
    "\n",
    "cluster_summary = pd.DataFrame(summary_rows).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nBest Models by Cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Plot cluster performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_summary_plot = cluster_summary.copy()\n",
    "cluster_summary_plot['Cluster_Model'] = cluster_summary_plot.apply(\n",
    "    lambda x: f\"Cluster {x['Cluster']}: {x['Best Model']}\", axis=1\n",
    ")\n",
    "sns.barplot(x='RMSE', y='Cluster_Model', data=cluster_summary_plot)\n",
    "plt.title('Best Model Performance by Cluster (RMSE - lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/training/best_model_performance_by_cluster.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b05184",
   "metadata": {},
   "source": [
    "## 2. Citywide Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01986065",
   "metadata": {},
   "source": [
    "--> Bei gutem Wetter fahren mehr Menschen Fahrrad\n",
    "\n",
    "Out:\n",
    "- Feature importance\n",
    "- Coeff\n",
    "\n",
    "Train models on all data, ignoring cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train_city = ['linear', 'lasso', 'ridge', 'decision_tree', 'xgboost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to train (citywide): ['linear', 'lasso', 'ridge', 'polynomial', 'decision_tree', 'xgboost']\n",
      "Dropped 0 rows with NaN values.\n",
      "\n",
      "Citywide training set features shape: (521618, 19)\n",
      "Citywide training set target shape: (521618,)\n",
      "Citywide test set features shape: (27454, 19)\n",
      "Citywide test set target shape: (27454,)\n",
      "\n",
      "==================================================\n",
      "Training models for the entire city\n",
      "==================================================\n",
      "\n",
      "Training linear model...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "  Best parameters: {'preprocessing': ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('num', StandardScaler(),\n",
      "                                 ['temperature_2m', 'rain', 'snowfall',\n",
      "                                  'cloud_cover', 'wind_speed_10m',\n",
      "                                  'num_bikes_available', 'latitude',\n",
      "                                  'longitude']),\n",
      "                                ('cat',\n",
      "                                 OneHotEncoder(handle_unknown='ignore',\n",
      "                                               sparse_output=False),\n",
      "                                 ['isHoliday', 'has_kiosk', 'weather_cluster',\n",
      "                                  'workhours', 'commute', 'free', 'night']),\n",
      "                                ('sin_mo...\n",
      "                                                     func=<function cos_transformer.<locals>._transform at 0x122e84680>,\n",
      "                                                     validate=True),\n",
      "                                 ['month']),\n",
      "                                ('cos_hour',\n",
      "                                 FunctionTransformer(feature_names_out='one-to-one',\n",
      "                                                     func=<function cos_transformer.<locals>._transform at 0x122e84cc0>,\n",
      "                                                     validate=True),\n",
      "                                 ['hour']),\n",
      "                                ('cos_weekday',\n",
      "                                 FunctionTransformer(feature_names_out='one-to-one',\n",
      "                                                     func=<function cos_transformer.<locals>._transform at 0x122e86ac0>,\n",
      "                                                     validate=True),\n",
      "                                 ['weekday'])])}\n",
      "  Preprocessing strategy: SinCos\n",
      "  MSE: 2.4786\n",
      "  RMSE: 1.5744\n",
      "  Mean Train Score: 3.4148889449242774\n",
      "\n",
      "Training lasso model...\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name_item \u001b[38;5;129;01min\u001b[39;00m models_to_train_city:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name_item \u001b[38;5;129;01min\u001b[39;00m pipelines_city:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         city_best_models[model_name_item], city_results[model_name_item] = \u001b[43mtrain_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcitywide\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using 'citywide' as pseudo cluster_id\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_item\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpipelines_city\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name_item\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparam_grids_city\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name_item\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX_city\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m            \u001b[49m\u001b[43my_city\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_item\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms not defined in pipelines_city\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_evaluate_model\u001b[39m\u001b[34m(cluster_id, subset, name, pipeline, param_grid, X, y, n_splits)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# GridSearch with time series split\u001b[39;00m\n\u001b[32m     11\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     12\u001b[39m     pipeline, \n\u001b[32m     13\u001b[39m     param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     return_train_score=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m best_params = grid_search.best_params_\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m best_params.keys():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bike-project/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "print(f\"Models to train (citywide): {models_to_train_city}\")\n",
    "\n",
    "current_feature_cols_citywide = copy.deepcopy(FEATURE_COLS)\n",
    "\n",
    "if 'cluster' not in current_feature_cols_citywide['drop']:\n",
    "    current_feature_cols_citywide['drop'].append('cluster')\n",
    "    print(\"Removed 'cluster' from drop cols.\")\n",
    "\n",
    "for cat_list_name in ['categorical', 'numerical', 'time']:\n",
    "    if cat_list_name in current_feature_cols_citywide and 'cluster' in current_feature_cols_citywide[cat_list_name]:\n",
    "        current_feature_cols_citywide[cat_list_name].remove('cluster')\n",
    "        print(f\"Removed 'cluster' from {cat_list_name} cols.\")\n",
    "\n",
    "X_city, y_city, X_city_test, y_city_test, datetime_col_city = prepare_data(\n",
    "    df,\n",
    "    CONFIG['target_col'],\n",
    "    current_feature_cols_citywide['categorical'],\n",
    "    current_feature_cols_citywide['numerical'],\n",
    "    current_feature_cols_citywide['time'],\n",
    "    current_feature_cols_citywide['drop']\n",
    ")\n",
    "\n",
    "print(f\"\\nCitywide training set features shape: {X_city.shape}\")\n",
    "print(f\"Citywide training set target shape: {y_city.shape}\")\n",
    "print(f\"Citywide test set features shape: {X_city_test.shape}\")\n",
    "print(f\"Citywide test set target shape: {y_city_test.shape}\")\n",
    "\n",
    "city_best_models = {}\n",
    "city_results = {}\n",
    "\n",
    "pipelines_city, preprocessing_strategies_city = create_model_pipelines(\n",
    "    current_feature_cols_citywide['categorical'],\n",
    "    current_feature_cols_citywide['numerical'],\n",
    "    current_feature_cols_citywide['time'],\n",
    "    X_city\n",
    ")\n",
    "param_grids_city = get_param_grids(preprocessing_strategies_city)\n",
    "\n",
    "print(f\"\\n{'='*50}\\nTraining models for the entire city\\n{'='*50}\")\n",
    "for model_name_item in models_to_train_city:\n",
    "    if model_name_item in pipelines_city:\n",
    "        city_best_models[model_name_item], city_results[model_name_item] = train_evaluate_model(\n",
    "            'citywide',  # Using 'citywide' as pseudo cluster_id\n",
    "            False,\n",
    "            model_name_item,\n",
    "            pipelines_city[model_name_item],\n",
    "            param_grids_city[model_name_item],\n",
    "            X_city,\n",
    "            y_city\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping {model_name_item} as it's not defined in pipelines_city\")\n",
    "\n",
    "# Create and display comparison DataFrame for citywide models\n",
    "if city_results:\n",
    "    city_comparison_df = create_comparison_df(city_results)\n",
    "    print(\"\\nModel Comparison Citywide:\")\n",
    "    print(city_comparison_df)\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=city_comparison_df.sort_values('RMSE', ascending=True))\n",
    "    plt.title('Citywide Model Comparison (RMSE - lower is better)')\n",
    "    plt.savefig('figures/training/citywide_model_comparison.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name_item in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name_item in city_best_models:\n",
    "            print(f\"\\nFeature Importance for {model_name_item} (Citywide):\")\n",
    "            plot_feature_importance(model_name_item, city_best_models[model_name_item], X_city, 'citywide')\n",
    "\n",
    "    # Show coefficients for linear models\n",
    "    for model_name_item in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name_item in city_best_models:\n",
    "            print(f\"\\nCoefficients for {model_name_item} (Citywide):\")\n",
    "            plot_coefficients(model_name_item, city_best_models[model_name_item], 'citywide')\n",
    "    \n",
    "    if not city_comparison_df.empty:\n",
    "        best_city_model_name = city_comparison_df['Model'].iloc[0]\n",
    "        best_city_model_rmse_val = city_comparison_df['RMSE'].iloc[0]\n",
    "        print(f\"\\nBest model for the City (based on validation RMSE): {best_city_model_name} with Validation RMSE: {best_city_model_rmse_val:.4f}\")\n",
    "\n",
    "        # Test best model on the citywide test set\n",
    "        y_city_pred_test = city_best_models[best_city_model_name].predict(X_city_test)\n",
    "        city_test_rmse = np.sqrt(mean_squared_error(y_city_test, y_city_pred_test))\n",
    "        r2_city_test = r2_score(y_city_test, y_city_pred_test)\n",
    "        print(f\"Best city model ({best_city_model_name}) on Test Set: RMSE: {city_test_rmse:.4f}, RÂ²: {r2_city_test:.4f}\")\n",
    "\n",
    "        print(f\"\\nVisualizing predictions on test set for {best_city_model_name} (Citywide):\")\n",
    "        # datetime_col_city[0] is train datetime, datetime_col_city[1] is test datetime\n",
    "        visualize_predictions(best_city_model_name, city_best_models[best_city_model_name], X_city_test, y_city_test, datetime_col_city[1], 'citywide')\n",
    "    else:\n",
    "        print(\"No models for citywide\")\n",
    "else:\n",
    "    print(\"No results for citywide\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae469c",
   "metadata": {},
   "source": [
    "## 3. Subset Models (May 10-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_may_subset: (194832, 94)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hour",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "departures",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "arrivals",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour_extract",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "temperature_2m",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "weather_code",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "rain",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "precipitation",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "snowfall",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "cloud_cover",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "wind_gusts_10m",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "wind_speed_10m",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "sunrise",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sunset",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weather_cluster",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_docks_available",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_bikes_available",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_ebikes_available",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "has_kiosk",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "capacity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "isHoliday",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "weekday",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "night",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "workhours",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "commute",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "free",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "day",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "dayofweek",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "dayofyear",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "delta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_delta_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_delta_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_arrivals_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_arrivals_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_departures_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_departures_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station_night",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station_nonnight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station_holiday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station_commute",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station_free",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_sum_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "var_sum_station_unfriendly_weather",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cluster",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e7adc6a2-f1dc-4b98-b6b7-2d39713f8d72",
       "rows": [
        [
         "3096",
         "10th & E St NW",
         "0",
         "0.0",
         "0.0",
         "0",
         "10.486",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "9.72",
         "4.829907",
         "1683712824.0",
         "1683763744.0",
         "clear_and_cloudy",
         "1",
         "12",
         "6",
         "YES",
         "15",
         "38.895914",
         "-77.026064",
         "False",
         "2",
         "True",
         "False",
         "False",
         "False",
         "10",
         "5",
         "2023",
         "2",
         "130",
         "2023-05-10 00:00:00+00:00",
         "0.0",
         "4.696688858762673",
         "0.09400638103919781",
         "1.3966499544211486",
         "1.3026435733819508",
         "4.562992764519648",
         "4.2928978540282206",
         "-0.14068672136128837",
         "1.0552772182400385",
         "0.24096019446976602",
         "0.5483035154363141",
         "0.38164691583105437",
         "1.0907755239258927",
         "0.23482224247948952",
         "6.82923941496837",
         "2.090063810391978",
         "5.689917665511752",
         "1.8552415679124885",
         "5.40026655815509",
         "0.1794871794871795",
         "4.044851183114849",
         "1.419871794871795",
         "4.733108665182616",
         "1.2403846153846154",
         "4.7812577294088525",
         "0.41449934980494146",
         "7.8557766923572885",
         "2.203511053315995",
         "6.254504741666402",
         "1.7890117035110533",
         "6.152217429456481",
         "0.14870689655172414",
         "3.7493479499919458",
         "1.3178879310344827",
         "4.413822918086413",
         "1.1691810344827587",
         "3.7625999758385196",
         "-0.060723514211886306",
         "4.250080992889332",
         "1.2454780361757105",
         "3.694992468496267",
         "1.306201550387597",
         "4.05742813342284",
         "0.0",
         "2.6992935278030994",
         "13.015092378333021",
         "0.6226071103008204",
         "2.222880860484382",
         "3.9453053783044667",
         "15.351129032365286",
         "2.66025641025641",
         "14.983881606068095",
         "3.9925227568270483",
         "16.957667649888478",
         "2.4870689655172415",
         "12.6034978378579",
         "2.5516795865633073",
         "11.25476021094892",
         "5"
        ],
        [
         "3097",
         "10th & E St NW",
         "1",
         "0.0",
         "0.0",
         "1",
         "10.386001",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "10.08",
         "5.8048253",
         "1683712824.0",
         "1683763744.0",
         "clear_and_cloudy",
         "1",
         "12",
         "6",
         "YES",
         "15",
         "38.895914",
         "-77.026064",
         "False",
         "2",
         "True",
         "False",
         "False",
         "False",
         "10",
         "5",
         "2023",
         "2",
         "130",
         "2023-05-10 01:00:00+00:00",
         "0.0",
         "4.696688858762673",
         "0.09400638103919781",
         "1.3966499544211486",
         "1.3026435733819508",
         "4.562992764519648",
         "4.2928978540282206",
         "-0.14068672136128837",
         "1.0552772182400385",
         "0.24096019446976602",
         "0.5483035154363141",
         "0.38164691583105437",
         "1.0907755239258927",
         "0.23482224247948952",
         "6.82923941496837",
         "2.090063810391978",
         "5.689917665511752",
         "1.8552415679124885",
         "5.40026655815509",
         "0.1794871794871795",
         "4.044851183114849",
         "1.419871794871795",
         "4.733108665182616",
         "1.2403846153846154",
         "4.7812577294088525",
         "0.41449934980494146",
         "7.8557766923572885",
         "2.203511053315995",
         "6.254504741666402",
         "1.7890117035110533",
         "6.152217429456481",
         "0.14870689655172414",
         "3.7493479499919458",
         "1.3178879310344827",
         "4.413822918086413",
         "1.1691810344827587",
         "3.7625999758385196",
         "-0.060723514211886306",
         "4.250080992889332",
         "1.2454780361757105",
         "3.694992468496267",
         "1.306201550387597",
         "4.05742813342284",
         "0.0",
         "2.6992935278030994",
         "13.015092378333021",
         "0.6226071103008204",
         "2.222880860484382",
         "3.9453053783044667",
         "15.351129032365286",
         "2.66025641025641",
         "14.983881606068095",
         "3.9925227568270483",
         "16.957667649888478",
         "2.4870689655172415",
         "12.6034978378579",
         "2.5516795865633073",
         "11.25476021094892",
         "5"
        ],
        [
         "3098",
         "10th & E St NW",
         "2",
         "0.0",
         "0.0",
         "2",
         "10.336",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "14.759999",
         "8.427383",
         "1683712824.0",
         "1683763744.0",
         "clear_and_cloudy",
         "1",
         "12",
         "6",
         "YES",
         "15",
         "38.895914",
         "-77.026064",
         "False",
         "2",
         "True",
         "False",
         "False",
         "False",
         "10",
         "5",
         "2023",
         "2",
         "130",
         "2023-05-10 02:00:00+00:00",
         "0.0",
         "4.696688858762673",
         "0.09400638103919781",
         "1.3966499544211486",
         "1.3026435733819508",
         "4.562992764519648",
         "4.2928978540282206",
         "-0.14068672136128837",
         "1.0552772182400385",
         "0.24096019446976602",
         "0.5483035154363141",
         "0.38164691583105437",
         "1.0907755239258927",
         "0.23482224247948952",
         "6.82923941496837",
         "2.090063810391978",
         "5.689917665511752",
         "1.8552415679124885",
         "5.40026655815509",
         "0.1794871794871795",
         "4.044851183114849",
         "1.419871794871795",
         "4.733108665182616",
         "1.2403846153846154",
         "4.7812577294088525",
         "0.41449934980494146",
         "7.8557766923572885",
         "2.203511053315995",
         "6.254504741666402",
         "1.7890117035110533",
         "6.152217429456481",
         "0.14870689655172414",
         "3.7493479499919458",
         "1.3178879310344827",
         "4.413822918086413",
         "1.1691810344827587",
         "3.7625999758385196",
         "-0.060723514211886306",
         "4.250080992889332",
         "1.2454780361757105",
         "3.694992468496267",
         "1.306201550387597",
         "4.05742813342284",
         "0.0",
         "2.6992935278030994",
         "13.015092378333021",
         "0.6226071103008204",
         "2.222880860484382",
         "3.9453053783044667",
         "15.351129032365286",
         "2.66025641025641",
         "14.983881606068095",
         "3.9925227568270483",
         "16.957667649888478",
         "2.4870689655172415",
         "12.6034978378579",
         "2.5516795865633073",
         "11.25476021094892",
         "5"
        ],
        [
         "3099",
         "10th & E St NW",
         "3",
         "0.0",
         "0.0",
         "3",
         "10.486",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "17.28",
         "10.086427",
         "1683712824.0",
         "1683763744.0",
         "clear_and_cloudy",
         "1",
         "12",
         "6",
         "YES",
         "15",
         "38.895914",
         "-77.026064",
         "False",
         "2",
         "True",
         "False",
         "False",
         "False",
         "10",
         "5",
         "2023",
         "2",
         "130",
         "2023-05-10 03:00:00+00:00",
         "0.0",
         "4.696688858762673",
         "0.09400638103919781",
         "1.3966499544211486",
         "1.3026435733819508",
         "4.562992764519648",
         "4.2928978540282206",
         "-0.14068672136128837",
         "1.0552772182400385",
         "0.24096019446976602",
         "0.5483035154363141",
         "0.38164691583105437",
         "1.0907755239258927",
         "0.23482224247948952",
         "6.82923941496837",
         "2.090063810391978",
         "5.689917665511752",
         "1.8552415679124885",
         "5.40026655815509",
         "0.1794871794871795",
         "4.044851183114849",
         "1.419871794871795",
         "4.733108665182616",
         "1.2403846153846154",
         "4.7812577294088525",
         "0.41449934980494146",
         "7.8557766923572885",
         "2.203511053315995",
         "6.254504741666402",
         "1.7890117035110533",
         "6.152217429456481",
         "0.14870689655172414",
         "3.7493479499919458",
         "1.3178879310344827",
         "4.413822918086413",
         "1.1691810344827587",
         "3.7625999758385196",
         "-0.060723514211886306",
         "4.250080992889332",
         "1.2454780361757105",
         "3.694992468496267",
         "1.306201550387597",
         "4.05742813342284",
         "0.0",
         "2.6992935278030994",
         "13.015092378333021",
         "0.6226071103008204",
         "2.222880860484382",
         "3.9453053783044667",
         "15.351129032365286",
         "2.66025641025641",
         "14.983881606068095",
         "3.9925227568270483",
         "16.957667649888478",
         "2.4870689655172415",
         "12.6034978378579",
         "2.5516795865633073",
         "11.25476021094892",
         "5"
        ],
        [
         "3100",
         "10th & E St NW",
         "4",
         "0.0",
         "0.0",
         "4",
         "10.236",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "18.0",
         "8.905908",
         "1683712824.0",
         "1683763744.0",
         "clear_and_cloudy",
         "1",
         "12",
         "6",
         "YES",
         "15",
         "38.895914",
         "-77.026064",
         "False",
         "2",
         "True",
         "False",
         "False",
         "False",
         "10",
         "5",
         "2023",
         "2",
         "130",
         "2023-05-10 04:00:00+00:00",
         "0.0",
         "4.696688858762673",
         "0.09400638103919781",
         "1.3966499544211486",
         "1.3026435733819508",
         "4.562992764519648",
         "4.2928978540282206",
         "-0.14068672136128837",
         "1.0552772182400385",
         "0.24096019446976602",
         "0.5483035154363141",
         "0.38164691583105437",
         "1.0907755239258927",
         "0.23482224247948952",
         "6.82923941496837",
         "2.090063810391978",
         "5.689917665511752",
         "1.8552415679124885",
         "5.40026655815509",
         "0.1794871794871795",
         "4.044851183114849",
         "1.419871794871795",
         "4.733108665182616",
         "1.2403846153846154",
         "4.7812577294088525",
         "0.41449934980494146",
         "7.8557766923572885",
         "2.203511053315995",
         "6.254504741666402",
         "1.7890117035110533",
         "6.152217429456481",
         "0.14870689655172414",
         "3.7493479499919458",
         "1.3178879310344827",
         "4.413822918086413",
         "1.1691810344827587",
         "3.7625999758385196",
         "-0.060723514211886306",
         "4.250080992889332",
         "1.2454780361757105",
         "3.694992468496267",
         "1.306201550387597",
         "4.05742813342284",
         "0.0",
         "2.6992935278030994",
         "13.015092378333021",
         "0.6226071103008204",
         "2.222880860484382",
         "3.9453053783044667",
         "15.351129032365286",
         "2.66025641025641",
         "14.983881606068095",
         "3.9925227568270483",
         "16.957667649888478",
         "2.4870689655172415",
         "12.6034978378579",
         "2.5516795865633073",
         "11.25476021094892",
         "5"
        ]
       ],
       "shape": {
        "columns": 94,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>hour</th>\n",
       "      <th>departures</th>\n",
       "      <th>arrivals</th>\n",
       "      <th>hour_extract</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>rain</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>...</th>\n",
       "      <th>var_sum_station_nonnight</th>\n",
       "      <th>avg_sum_station_holiday</th>\n",
       "      <th>var_sum_station_holiday</th>\n",
       "      <th>avg_sum_station_commute</th>\n",
       "      <th>var_sum_station_commute</th>\n",
       "      <th>avg_sum_station_free</th>\n",
       "      <th>var_sum_station_free</th>\n",
       "      <th>avg_sum_station_unfriendly_weather</th>\n",
       "      <th>var_sum_station_unfriendly_weather</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>10th &amp; E St NW</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.486000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.351129</td>\n",
       "      <td>2.660256</td>\n",
       "      <td>14.983882</td>\n",
       "      <td>3.992523</td>\n",
       "      <td>16.957668</td>\n",
       "      <td>2.487069</td>\n",
       "      <td>12.603498</td>\n",
       "      <td>2.55168</td>\n",
       "      <td>11.25476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>10th &amp; E St NW</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.386001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.351129</td>\n",
       "      <td>2.660256</td>\n",
       "      <td>14.983882</td>\n",
       "      <td>3.992523</td>\n",
       "      <td>16.957668</td>\n",
       "      <td>2.487069</td>\n",
       "      <td>12.603498</td>\n",
       "      <td>2.55168</td>\n",
       "      <td>11.25476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>10th &amp; E St NW</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.336000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.351129</td>\n",
       "      <td>2.660256</td>\n",
       "      <td>14.983882</td>\n",
       "      <td>3.992523</td>\n",
       "      <td>16.957668</td>\n",
       "      <td>2.487069</td>\n",
       "      <td>12.603498</td>\n",
       "      <td>2.55168</td>\n",
       "      <td>11.25476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>10th &amp; E St NW</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.486000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.351129</td>\n",
       "      <td>2.660256</td>\n",
       "      <td>14.983882</td>\n",
       "      <td>3.992523</td>\n",
       "      <td>16.957668</td>\n",
       "      <td>2.487069</td>\n",
       "      <td>12.603498</td>\n",
       "      <td>2.55168</td>\n",
       "      <td>11.25476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>10th &amp; E St NW</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10.236000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.351129</td>\n",
       "      <td>2.660256</td>\n",
       "      <td>14.983882</td>\n",
       "      <td>3.992523</td>\n",
       "      <td>16.957668</td>\n",
       "      <td>2.487069</td>\n",
       "      <td>12.603498</td>\n",
       "      <td>2.55168</td>\n",
       "      <td>11.25476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_name  hour  departures  arrivals  hour_extract  \\\n",
       "3096  10th & E St NW     0         0.0       0.0             0   \n",
       "3097  10th & E St NW     1         0.0       0.0             1   \n",
       "3098  10th & E St NW     2         0.0       0.0             2   \n",
       "3099  10th & E St NW     3         0.0       0.0             3   \n",
       "3100  10th & E St NW     4         0.0       0.0             4   \n",
       "\n",
       "      temperature_2m  weather_code  rain  precipitation  snowfall  ...  \\\n",
       "3096       10.486000           0.0   0.0            0.0       0.0  ...   \n",
       "3097       10.386001           0.0   0.0            0.0       0.0  ...   \n",
       "3098       10.336000           0.0   0.0            0.0       0.0  ...   \n",
       "3099       10.486000           0.0   0.0            0.0       0.0  ...   \n",
       "3100       10.236000           0.0   0.0            0.0       0.0  ...   \n",
       "\n",
       "      var_sum_station_nonnight  avg_sum_station_holiday  \\\n",
       "3096                 15.351129                 2.660256   \n",
       "3097                 15.351129                 2.660256   \n",
       "3098                 15.351129                 2.660256   \n",
       "3099                 15.351129                 2.660256   \n",
       "3100                 15.351129                 2.660256   \n",
       "\n",
       "      var_sum_station_holiday  avg_sum_station_commute  \\\n",
       "3096                14.983882                 3.992523   \n",
       "3097                14.983882                 3.992523   \n",
       "3098                14.983882                 3.992523   \n",
       "3099                14.983882                 3.992523   \n",
       "3100                14.983882                 3.992523   \n",
       "\n",
       "      var_sum_station_commute avg_sum_station_free  var_sum_station_free  \\\n",
       "3096                16.957668             2.487069             12.603498   \n",
       "3097                16.957668             2.487069             12.603498   \n",
       "3098                16.957668             2.487069             12.603498   \n",
       "3099                16.957668             2.487069             12.603498   \n",
       "3100                16.957668             2.487069             12.603498   \n",
       "\n",
       "      avg_sum_station_unfriendly_weather  var_sum_station_unfriendly_weather  \\\n",
       "3096                             2.55168                            11.25476   \n",
       "3097                             2.55168                            11.25476   \n",
       "3098                             2.55168                            11.25476   \n",
       "3099                             2.55168                            11.25476   \n",
       "3100                             2.55168                            11.25476   \n",
       "\n",
       "     cluster  \n",
       "3096       5  \n",
       "3097       5  \n",
       "3098       5  \n",
       "3099       5  \n",
       "3100       5  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: this obviously only works if the df is not sampled via CONFIG['sample'] = True\n",
    "\n",
    "df_may = df[\n",
    "    (df['timestamp'].dt.month == 5) &\n",
    "    (df['timestamp'].dt.day >= 10) & \n",
    "    (df['timestamp'].dt.day <= 20)\n",
    "].copy()\n",
    "\n",
    "print(f\"Shape of df_may_subset: {df_may.shape}\")\n",
    "df_may.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c10d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest first to detect potential errors early\n",
    "models_to_train_cluster_may = ['random_forest', 'linear', 'lasso', 'ridge', 'decision_tree', 'xgboost']\n",
    "\n",
    "unique_clusters = sorted(df_may['cluster'].unique().tolist())\n",
    "print(f\"Found {len(unique_clusters)} clusters: {unique_clusters}\")\n",
    "\n",
    "# # Store cluster results\n",
    "all_cluster_models = {}\n",
    "all_cluster_results = {}\n",
    "all_cluster_comparisons = {}\n",
    "\n",
    "# Train models for each cluster\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    print(f\"\\n{'='*50}\\nProcessing Cluster {cluster_id}\\n{'='*50}\")\n",
    "    \n",
    "    # Get data for this cluster\n",
    "    X_cluster, y_cluster, X_cluster_test, y_cluster_test, datetime_col = prepare_cluster_data(cluster_id, df_may)\n",
    "    print(f\"Cluster size: {len(X_cluster)} records\")\n",
    "    \n",
    "    # Train models\n",
    "    models, results = train_cluster_models(cluster_id, X_cluster, y_cluster, models_to_train_cluster, subset=True)\n",
    "\n",
    "    # Store results\n",
    "    all_cluster_models[cluster_id] = models\n",
    "    all_cluster_results[cluster_id] = results\n",
    "    all_cluster_comparisons[cluster_id] = create_comparison_df(results)\n",
    "            \n",
    "    print(f\"\\nModel Comparison for Cluster {cluster_id}:\")\n",
    "    comparison_cluster = create_comparison_df(results)\n",
    "    print(comparison_cluster)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=comparison_cluster)\n",
    "    plt.title(f'Cluster {cluster_id} - Model Comparison (RMSE - lower is better)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/training/may_subset/cluster_{cluster_id}_model_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nFeature Importance for {model_name} in Cluster {cluster_id}:\")\n",
    "            feature_importance = plot_feature_importance(model_name, models[model_name], X_cluster, 'may_subset')\n",
    "\n",
    "    # \"feature importance\" for regression methods\n",
    "    for model_name in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nCoefficients for {model_name} in Cluster {cluster_id}:\")\n",
    "            coefficients = plot_coefficients(model_name, models[model_name], cluster_id)\n",
    "    \n",
    "    # Visualize best model predictions\n",
    "    best_model_name = comparison_cluster['Model'].iloc[0]\n",
    "    print(f\"Best model for Cluster {cluster_id}: {best_model_name} with validation RMSE: {comparison_cluster['RMSE'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Test best model on test set\n",
    "    y_cluster_pred = models[best_model_name].predict(X_cluster_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_cluster_test, y_cluster_pred))\n",
    "    print(f\"Best model on test set for Cluster {cluster_id}: {best_model_name} with test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(f\"\\nVisualizing predictions on test set (5% of data) for {best_model_name} in Cluster {cluster_id}:\")\n",
    "    pred_df = visualize_predictions(best_model_name, models[best_model_name], X_cluster_test, y_cluster_test, datetime_col[1], 'may_subset') # datetime_col[1] are times corresponding to test set\n",
    "\n",
    "    if i == CONFIG['top_n_clusters']-1:\n",
    "        print(f\"Reached top {CONFIG['top_n_clusters']} clusters. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of best models across clusters\n",
    "summary_rows = []\n",
    "for cluster_id in all_cluster_comparisons:\n",
    "    best_model = all_cluster_comparisons[cluster_id].iloc[0]\n",
    "    summary_rows.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Best Model': best_model['Model'],\n",
    "        'RMSE': best_model['RMSE'],\n",
    "    })\n",
    "\n",
    "cluster_summary = pd.DataFrame(summary_rows).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nBest Models by Cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Plot cluster performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_summary_plot = cluster_summary.copy()\n",
    "cluster_summary_plot['Cluster_Model'] = cluster_summary_plot.apply(\n",
    "    lambda x: f\"Cluster {x['Cluster']}: {x['Best Model']}\", axis=1\n",
    ")\n",
    "sns.barplot(x='RMSE', y='Cluster_Model', data=cluster_summary_plot)\n",
    "plt.title('Best Model Performance by Cluster (RMSE - lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afde712",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
