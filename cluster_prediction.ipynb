{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import random\n",
    "#import pymc as pm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af7147",
   "metadata": {},
   "source": [
    "# CONFIG\n",
    "\n",
    "**Set sample to False if you want to run algos on entire dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'sample': True,\n",
    "    'sample_months': 1, # how many months to sample (for development)\n",
    "    'top_n_clusters': 1, # how many clusters to train\n",
    "    'random_state': 123,\n",
    "    'target_col': 'departures',\n",
    "    'n_jobs': 4,  # gridsearch parallelization, might need to adjust based on your system\n",
    "    'ts_splits': 5, # TimeSeriesSplit number of splits\n",
    "    'ts_gap': 48,  # 2-day gap\n",
    "    'visualize_clusters': False\n",
    "}\n",
    "\n",
    "FEATURE_COLS = {\n",
    "    'categorical': ['isHoliday', 'has_kiosk', 'weather_cluster', 'workhours', 'commute', 'free', 'night'],\n",
    "    'drop': ['sum', 'weather_code', 'timestamp', 'station_name', 'arrivals', 'num_docks_available', 'num_ebikes_available', 'capacity', 'cluster', 'sunset', 'sunrise', 'year', 'hour_extract', 'precipitation', 'wind_gusts_10m', 'dayofyear', 'dayofweek', 'delta'],\n",
    "    'time': ['weekday', 'day', 'month', 'hour']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95c97e",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/final/df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e326de",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS[\"drop\"] = FEATURE_COLS[\"drop\"] + [col for col in df.columns if (col.startswith(\"var\") or col.startswith(\"avg\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee707dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline-Performance for 0-heavy Data:\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df[df[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over all time cluster {i}: {baseline}\")\n",
    "print(\"---\")\n",
    "# TODO: Do we need to update this to conform with the \"primary\" train-test-split? (The one at the very top)\n",
    "baseline = np.sqrt((sum(df[\"departures\"]**2))/len(df))\n",
    "print(f\"Baseline over all time whole city: {baseline}\")\n",
    "print(\"---\")\n",
    "df_may_subset = df[(df[\"month\"]==5) & (df[\"day\"]>=10) & (df[\"day\"]<=20)].copy()\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df_may_subset[df_may_subset[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over 10-20 may cluster {i}: {baseline}\")\n",
    "df_may_subset = df[(df[\"month\"]==5) & (df[\"day\"]>=21) & (df[\"day\"]<=30)].copy()\n",
    "for i in range(0,10):\n",
    "    df_cluster_subset = df_may_subset[df_may_subset[\"cluster\"]==i]\n",
    "    baseline = np.sqrt((sum(df_cluster_subset[\"departures\"]**2))/len(df_cluster_subset))\n",
    "    print(f\"Baseline over 21-30 may cluster {i}: {baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ca495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCE DATASET SIZE FOR DEVELOPMENT\n",
    "if CONFIG['sample']:\n",
    "    months = df['timestamp'].dt.month.unique()\n",
    "    random_month = random.sample(list(months), CONFIG['sample_months'])\n",
    "    df = df[df['timestamp'].dt.month.isin(random_month)]\n",
    "    print(f'Sampled {CONFIG[\"sample_months\"]} month(s): {random_month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All remaining columns are considered numerical\n",
    "FEATURE_COLS['numerical'] = [col for col in df.columns if col not in ([CONFIG['target_col']] + FEATURE_COLS['categorical'] + FEATURE_COLS['drop'] + FEATURE_COLS['time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for features\n",
    "print(\"Categorical:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['categorical']))\n",
    "print(\"Drop:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['drop']))\n",
    "print(\"Time:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['time']))\n",
    "print(\"Numerical:\\n\" + '\\n'.join(f\"  - {feature_col}\" for feature_col in FEATURE_COLS['numerical']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(df[CONFIG['target_col']])\n",
    "plt.title(f'Distribution of {CONFIG[\"target_col\"]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6553df3",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ca232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_data(df, target_col, categorical_cols, numerical_cols, time_cols, drop_cols):\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"Dropped {len(df) - len(df_clean)} rows with NaN values.\")\n",
    "\n",
    "    # Keep datetime for visualization purposes if available\n",
    "    datetime_col = df_clean['timestamp'] if 'timestamp' in df_clean.columns else None\n",
    "    datetime_keeper = df_clean.copy()\n",
    "\n",
    "    # Drop columns defined in drop_cols\n",
    "    df_clean = df_clean.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # Split features and target\n",
    "    X = df_clean[categorical_cols + numerical_cols + time_cols]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=CONFIG['random_state'], shuffle=False)\n",
    "\n",
    "    train_indices = X_train.index\n",
    "    test_indices = X_test.index\n",
    "    datetime_train = datetime_keeper.loc[train_indices, 'timestamp'] if datetime_col is not None else None\n",
    "    datetime_test = datetime_keeper.loc[test_indices, 'timestamp'] if datetime_col is not None else None\n",
    "    datetime_col = [datetime_train, datetime_test] if datetime_train is not None else None\n",
    "   \n",
    "    return X_train, y_train, X_test, y_test, datetime_col\n",
    "\n",
    "# Test-apply data preparation\n",
    "X, y, X_test, y_test, datetime_col = prepare_data(df, CONFIG['target_col'], FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], FEATURE_COLS['drop'])\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set:\")\n",
    "print(f\"Features shape: {X_test.shape}\")\n",
    "print(f\"Target shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20b0e3",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "# Function to create model pipelines for each cluster\n",
    "def create_model_pipelines(categorical_cols, numerical_cols, time_cols):\n",
    "    \n",
    "    preprocessor_plain = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    preprocessor_onehot = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        ('time', OneHotEncoder(handle_unknown='ignore', sparse_output=False), time_cols),\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    # Prepared but currently not in use\n",
    "    preprocessor_sincos = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        ('sin_month', sin_transformer(12), ['month']),\n",
    "        ('sin_hour', sin_transformer(24), ['hour']),\n",
    "        ('sin_weekday', sin_transformer(7), ['weekday']),\n",
    "        ('cos_month', cos_transformer(12), ['month']),\n",
    "        ('cos_hour', cos_transformer(24), ['hour']),\n",
    "        ('cos_weekday', cos_transformer(7), ['weekday'])\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    # Create pipelines\n",
    "    pipelines = {\n",
    "        'linear': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        \n",
    "        'lasso': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', Lasso(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        \n",
    "        'ridge': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', Ridge(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        \n",
    "        'decision_tree': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', DecisionTreeRegressor(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'random_forest': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', RandomForestRegressor(random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'xgboost': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', xgb.XGBRegressor(objective='reg:squarederror', random_state=CONFIG['random_state']))\n",
    "        ]),\n",
    "        'gbm': Pipeline([\n",
    "            ('preprocessing', 'passthrough'),\n",
    "            ('regressor', GradientBoostingRegressor(random_state=CONFIG['random_state']))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Store preprocessor strategies\n",
    "    preprocessing_strategies = {\n",
    "        'plain': preprocessor_plain,\n",
    "        'onehot': preprocessor_onehot,\n",
    "        'sincos': preprocessor_sincos\n",
    "    }\n",
    "    \n",
    "    preprocessor_plain.strategy_name = 'Plain'\n",
    "    preprocessor_onehot.strategy_name = 'OneHot'  \n",
    "    preprocessor_sincos.strategy_name = 'SinCos'\n",
    "    \n",
    "    return pipelines, preprocessing_strategies\n",
    "\n",
    "# Function to get parameter grids\n",
    "def get_param_grids(preprocessing_strategies):\n",
    "    return {\n",
    "        'linear': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ]\n",
    "            },\n",
    "        \n",
    "        'lasso': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__alpha': np.logspace(-4,4,20),\n",
    "            'regressor__max_iter': [1000, 2000]\n",
    "        },\n",
    "        \n",
    "        'ridge': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__alpha': np.logspace(-4,4,20)\n",
    "        },\n",
    "        \n",
    "        'decision_tree': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__max_depth': [3, 5, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10, 15, 20]\n",
    "        },\n",
    "        \n",
    "        'random_forest': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10, 15, 20]\n",
    "        },\n",
    "        \n",
    "        'xgboost': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [3, 6],\n",
    "            'regressor__learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        \n",
    "        'gbm': {\n",
    "            'preprocessing': [\n",
    "                preprocessing_strategies['plain'],\n",
    "                preprocessing_strategies['onehot'],\n",
    "                preprocessing_strategies['sincos']\n",
    "            ],\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [3, 6],\n",
    "            'regressor__learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04542997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(model_name, pipeline, cluster_id):\n",
    "    reg = pipeline.named_steps['regressor']\n",
    "            \n",
    "    feat_names = pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "            \n",
    "    # build and sort df\n",
    "    coef_df = (\n",
    "        pd.DataFrame({'feature': feat_names, 'coefficient': reg.coef_})\n",
    "            .assign(abs_coef=lambda df: df.coefficient.abs())\n",
    "            .sort_values('abs_coef', ascending=False)\n",
    "            .drop(columns='abs_coef')\n",
    "        )\n",
    "\n",
    "    # largest coefficients first\n",
    "    coef_df = coef_df[::-1]\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.scatter(coef_df['coefficient'], coef_df['feature'], s=50, color='C0')\n",
    "    plt.axvline(0, linestyle='--', color='gray')\n",
    "    plt.title(f'{model_name} Coefficients (Cluster {cluster_id})')\n",
    "    plt.xlabel('Coefficient value')\n",
    "    plt.ylabel('')\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07299b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature importance\n",
    "def plot_feature_importance(model_name, model, X):\n",
    "    # Extract the regressor from pipeline\n",
    "    regressor = None\n",
    "    for step_name, step in model.named_steps.items():\n",
    "        if hasattr(step, 'feature_importances_'):\n",
    "            regressor = step\n",
    "            break\n",
    "    \n",
    "    if regressor is None:\n",
    "        print(f\"Model {model_name} doesn't support feature importance.\")\n",
    "        return\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    try:\n",
    "        # Try to get preprocessed feature names\n",
    "        if 'preprocessing' in model.named_steps and hasattr(model['preprocessing'], 'get_feature_names_out'):\n",
    "            feature_names = model['preprocessing'].get_feature_names_out()\n",
    "        else:\n",
    "            # Fallback to original feature names or indices\n",
    "            feature_names = X.columns if hasattr(X, 'columns') else [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        # Ensure the lengths match\n",
    "        if len(feature_names) != len(regressor.feature_importances_):\n",
    "            print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match importances length ({len(regressor.feature_importances_)})\")\n",
    "            # Use indices as fallback\n",
    "            feature_names = [f\"feature_{i}\" for i in range(len(regressor.feature_importances_))]\n",
    "            \n",
    "        # Extract feature importances\n",
    "        importance = regressor.feature_importances_\n",
    "        \n",
    "        # Create DataFrame for better visualization\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Feature importance report for {model_name}:\")\n",
    "        print(feature_imp)\n",
    "        \n",
    "        return feature_imp\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make and visualize predictions\n",
    "def visualize_predictions(model_name, model, X, y, datetime_col):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    pred_df = pd.DataFrame({\n",
    "        'datetime': datetime_col,\n",
    "        'actual': y,\n",
    "        'predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(pred_df['datetime'], pred_df['actual'], label='Actual', alpha=0.7)\n",
    "    plt.plot(pred_df['datetime'], pred_df['predicted'], label='Predicted', alpha=0.7)\n",
    "    plt.title(f'{model_name} - Actual vs Predicted (MSE: {mse:.2f}, RÂ²: {r2:.2f})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot residuals\n",
    "    pred_df['residual'] = pred_df['actual'] - pred_df['predicted']\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.scatter(pred_df['predicted'], pred_df['residual'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title(f'{model_name} - Residuals Plot')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243ef9d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeModelMetricsOnDisc(name, results, cluster_id):\n",
    "    FILE = \"checkpoints/model_train_checkpointing.csv\"\n",
    "    file_exists = os.path.exists(FILE)\n",
    "\n",
    "    best_params = ' '.join(str(results['best_params']).replace('\\n', '').split())\n",
    "    time_string = '{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "\n",
    "    with open(FILE, 'a') as file:\n",
    "        if not file_exists:\n",
    "            file.write(\"timestamp,model,cluster_id,best_params,best_score,rmse,mean_train_score\\n\")\n",
    "        file.write(f\"{time_string},{name},{cluster_id},{best_params},{results['best_score']},{results['rmse']},{results['mean_train_score']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a single model\n",
    "def train_evaluate_model(cluster_id, name, pipeline, param_grid, X, y, n_splits=CONFIG['ts_splits']):\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    \n",
    "    # Use TimeSeriesSplit for validation\n",
    "    tscv = TimeSeriesSplit(\n",
    "        n_splits=n_splits,\n",
    "        gap=CONFIG['ts_gap'])\n",
    "    \n",
    "    # GridSearch with time series split\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid,\n",
    "        cv=tscv, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=CONFIG['n_jobs'],\n",
    "        verbose=1,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    if 'preprocessing' in best_params.keys():\n",
    "        if hasattr(best_params['preprocessing'], 'strategy_name'):\n",
    "            preproc_name = best_params['preprocessing'].strategy_name\n",
    "    else:\n",
    "        preproc_name = 'Unknown'\n",
    "    \n",
    "    # Store results\n",
    "    best_model = grid_search.best_estimator_\n",
    "    mean_train_scores = -grid_search.cv_results_['mean_train_score']\n",
    "    mean_train_score = np.mean(mean_train_scores)\n",
    "    result = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': -grid_search.best_score_,  # Convert back to positive MSE\n",
    "        'rmse': np.sqrt(-grid_search.best_score_),\n",
    "        'mean_train_score': mean_train_score,\n",
    "    }\n",
    "    \n",
    "    print(f\"  Best parameters: {result['best_params']}\")\n",
    "    print(f\"  Preprocessing strategy: {preproc_name}\")\n",
    "    print(f\"  MSE: {result['best_score']:.4f}\")\n",
    "    print(f\"  RMSE: {result['rmse']:.4f}\")\n",
    "    print(f\"  Mean Train Score: {mean_train_score}\")\n",
    "    storeModelMetricsOnDisc(name, result, cluster_id)\n",
    "\n",
    "    return best_model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875adcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cluster_data(cluster_id, df_processed):\n",
    "\n",
    "    cluster_df = df_processed[df_processed['cluster'] == cluster_id].copy()\n",
    "    return prepare_data(cluster_df, CONFIG['target_col'], FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'], FEATURE_COLS['drop'])\n",
    "\n",
    "def train_cluster_models(cluster_id, X, y, models_to_train):\n",
    "\n",
    "    best_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    pipelines, preprocessing_strategies = create_model_pipelines(FEATURE_COLS['categorical'], FEATURE_COLS['numerical'], FEATURE_COLS['time'])\n",
    "    param_grids = get_param_grids(preprocessing_strategies)\n",
    "    \n",
    "    for model_name in models_to_train:\n",
    "        if model_name in pipelines:\n",
    "            best_models[model_name], results[model_name] = train_evaluate_model(\n",
    "                cluster_id, model_name, pipelines[model_name], param_grids[model_name], X, y\n",
    "            )\n",
    "    \n",
    "    return best_models, results\n",
    "\n",
    "def create_comparison_df(results_dict):\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': list(results_dict.keys()),\n",
    "        'RMSE': [results_dict[m]['rmse'] for m in results_dict.keys()]\n",
    "    }).sort_values('RMSE')\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91651622",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = ['linear', 'lasso', 'ridge', 'polynomial', 'decision_tree', 'xgboost']\n",
    "\n",
    "unique_clusters = sorted(df['cluster'].unique().tolist())\n",
    "print(f\"Found {len(unique_clusters)} clusters: {unique_clusters}\")\n",
    "\n",
    "# # Store cluster results\n",
    "all_cluster_models = {}\n",
    "all_cluster_results = {}\n",
    "all_cluster_comparisons = {}\n",
    "\n",
    "# Train models for each cluster\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    print(f\"\\n{'='*50}\\nProcessing Cluster {cluster_id}\\n{'='*50}\")\n",
    "    \n",
    "    # Get data for this cluster\n",
    "    X_cluster, y_cluster, X_cluster_test, y_cluster_test, datetime_col = prepare_cluster_data(cluster_id, df) # TODO use test sets for validation\n",
    "    print(f\"Cluster size: {len(X_cluster)} records\")\n",
    "    \n",
    "    # Train models\n",
    "    models, results = train_cluster_models(cluster_id, X_cluster, y_cluster, models_to_train)\n",
    "    # time_string = '{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    # with open(f\"checkpoints/{cluster_id}-best_models-{time_string}.pkl\", 'wb') as outp:\n",
    "    #    pickle.dump([models, results], outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Store results\n",
    "    all_cluster_models[cluster_id] = models\n",
    "    all_cluster_results[cluster_id] = results\n",
    "    all_cluster_comparisons[cluster_id] = create_comparison_df(results)\n",
    "            \n",
    "    print(f\"\\nModel Comparison for Cluster {cluster_id}:\")\n",
    "    comparison_cluster = create_comparison_df(results)\n",
    "    print(comparison_cluster)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='RMSE', y='Model', data=comparison_cluster)\n",
    "    plt.title(f'Cluster {cluster_id} - Model Comparison (RMSE - lower is better)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show feature importance for tree-based models\n",
    "    for model_name in ['xgboost', 'random_forest', 'decision_tree']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nFeature Importance for {model_name} in Cluster {cluster_id}:\")\n",
    "            feature_importance = plot_feature_importance(model_name, models[model_name], X_cluster)\n",
    "\n",
    "    # \"feature importance\" for regression methods\n",
    "    for model_name in ['linear', 'lasso', 'ridge']:\n",
    "        if model_name in models:\n",
    "            print(f\"\\nCoefficients for {model_name} in Cluster {cluster_id}:\")\n",
    "            coefficients = plot_coefficients(model_name, models[model_name], cluster_id)\n",
    "    \n",
    "    # Visualize best model predictions\n",
    "    best_model_name = comparison_cluster['Model'].iloc[0]\n",
    "    print(f\"Best model for Cluster {cluster_id}: {best_model_name} with validation RMSE: {comparison_cluster['RMSE'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Test best model on test set\n",
    "    y_cluster_pred = models[best_model_name].predict(X_cluster_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_cluster_test, y_cluster_pred))\n",
    "    print(f\"Best model on test set for Cluster {cluster_id}: {best_model_name} with test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    print(f\"\\nVisualizing predictions on test set (5% of data) for {best_model_name} in Cluster {cluster_id}:\")\n",
    "    pred_df = visualize_predictions(best_model_name, models[best_model_name], X_cluster_test, y_cluster_test, datetime_col[1]) # datetime_col[1] are times corresponding to test set\n",
    "\n",
    "    if i == CONFIG['top_n_clusters']-1:\n",
    "        print(f\"Reached top {CONFIG['top_n_clusters']} clusters. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if CONFIG['visualize_clusters']:\n",
    "    visualize_cluster_results(cluster_id, models, X_cluster, y_cluster) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fe03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of best models across clusters\n",
    "summary_rows = []\n",
    "for cluster_id in all_cluster_comparisons:\n",
    "    best_model = all_cluster_comparisons[cluster_id].iloc[0]\n",
    "    summary_rows.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Best Model': best_model['Model'],\n",
    "        'RMSE': best_model['RMSE'],\n",
    "    })\n",
    "\n",
    "cluster_summary = pd.DataFrame(summary_rows).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nBest Models by Cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Plot cluster performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_summary_plot = cluster_summary.copy()\n",
    "cluster_summary_plot['Cluster_Model'] = cluster_summary_plot.apply(\n",
    "    lambda x: f\"Cluster {x['Cluster']}: {x['Best Model']}\", axis=1\n",
    ")\n",
    "sns.barplot(x='RMSE', y='Cluster_Model', data=cluster_summary_plot)\n",
    "plt.title('Best Model Performance by Cluster (RMSE - lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b05184",
   "metadata": {},
   "source": [
    "## Citywide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01986065",
   "metadata": {},
   "source": [
    "--> Bei gutem Wetter fahren mehr Menschen Fahrrad\n",
    "\n",
    "Out:\n",
    "- Feature importance\n",
    "- Coeff\n",
    "\n",
    "Train models on all data, ignoring cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = ['linear', 'lasso', 'ridge', 'polynomial', 'decision_tree', 'xgboost']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae469c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b67ff",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- Split data to range from May 10 - 20\n",
    "- Adjust checkpointing to include column about this split (bool)\n",
    "- Train **all** models on this data, including random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = ['linear', 'lasso', 'ridge', 'polynomial', 'decision_tree', 'xgboost', 'random_forest']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
